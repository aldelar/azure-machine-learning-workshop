{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Machine Learning Pipeline for Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Machine Learning and Pipeline SDK-specific Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.0.83\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "import azureml.dataprep\n",
    "from azureml.core import Workspace, Experiment, Datastore, Dataset\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.data import TabularDataset\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Workspace and Retrieve a Compute Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "create workspace"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Workspace:\n",
      "agd-mlws\n",
      "azure-ml-workshop\n",
      "westus2\n",
      "c5ec24ce-9c5f-4da2-bf12-9ca8e9758d60\n",
      "== Datastore: workspaceblobstore\n",
      "== Compute targets:\n",
      "  agd-inference\n",
      "  agd-inference-v\n",
      "  agd-training-gpu\n",
      "  agd-training-cpu\n",
      "== AML compute target attached: agd-training-cpu\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(\"== Workspace:\")\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n",
    "\n",
    "# Default datastore (Azure blob storage)\n",
    "# def_blob_store = ws.get_default_datastore()\n",
    "blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "print(\"== Datastore: {}\".format(blob_store.name))\n",
    "\n",
    "# list compute targets\n",
    "print(\"== Compute targets:\")\n",
    "for ct in ws.compute_targets:\n",
    "    print(\"  \" + ct)\n",
    "    \n",
    "# Retrieve a compute target    \n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "aml_compute_target = \"agd-training-cpu\"\n",
    "try:\n",
    "    aml_compute = AmlCompute(ws, aml_compute_target)\n",
    "    print(\"== AML compute target attached: \" + aml_compute_target)\n",
    "except ComputeTargetException:\n",
    "    print(\"== AML compute target not found: \" + aml_compute_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Compute Configuration\n",
    "\n",
    "This step uses a docker image, use a [**RunConfiguration**](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.runconfiguration?view=azure-ml-py) to specify these requirements and use when creating the PythonScriptStep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Run Configuration created\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# create a new runconfig object\n",
    "run_config = RunConfiguration()\n",
    "\n",
    "# enable Docker \n",
    "run_config.environment.docker.enabled = True\n",
    "\n",
    "# set Docker base image to the default CPU-based image\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "\n",
    "# use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# specify dependencies\n",
    "#run_config.environment.python.conda_dependencies = CondaDependencies.create(\n",
    "#    conda_packages=['pandas'],\n",
    "#    pip_packages=['azureml-sdk', 'azureml-dataprep[fuse,pandas]', 'azureml-train-automl'], \n",
    "#    pin_sdk_version=False)\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies(\n",
    "    conda_dependencies_file_path='data-prep-pipeline.yml')\n",
    "\n",
    "#\n",
    "print(\"== Run Configuration created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Pipeline Steps with Inputs and Outputs\n",
    "As mentioned earlier, a step in the pipeline can take data as input. This data can be a data source that lives in one of the accessible data locations, or intermediate data produced by a previous step in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines steps\n",
    "Machine learning pipelines can have many steps and these steps could use or reuse datasources and intermediate data. Here's how we construct such a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== PythonScriptStep featurization_step created\n"
     ]
    }
   ],
   "source": [
    "# The best practice is to use separate folders for scripts and its dependent files\n",
    "# for each step and specify that folder as the source_directory for the step.\n",
    "# This helps reduce the size of the snapshot created for the step (only the specific folder is snapshotted).\n",
    "# Since changes in any files in the source_directory would trigger a re-upload of the snapshot, this helps\n",
    "# keep the reuse of the step when there are no changes in the source_directory of the step.\n",
    "source_directory_featurization = 'src/featurization'\n",
    "\n",
    "# Input\n",
    "clustering_ds = Dataset.get_by_name(ws,\"clustering-training\")\n",
    "\n",
    "# Output\n",
    "clustering_featurized_ds = PipelineData(\"output\", datastore=blob_store).as_dataset()\n",
    "clustering_featurized_ds.register(name=\"clustering-training-featurized\", create_new_version=True)\n",
    "\n",
    "# Featurize date/hour columns\n",
    "featurization_step = PythonScriptStep(\n",
    "    script_name=\"featurize.py\", \n",
    "    arguments=[\"--date_column\",\"date\",\n",
    "               \"--hour_column\",\"he\",\n",
    "               \"--datetime_column_name\",\"DATETIME\",\n",
    "               \"--output\", clustering_featurized_ds],\n",
    "    inputs=[clustering_ds.as_named_input(\"input\")],\n",
    "    outputs=[clustering_featurized_ds],\n",
    "    compute_target=aml_compute,\n",
    "    source_directory=source_directory_featurization,\n",
    "    runconfig=run_config\n",
    ")\n",
    "print(\"== PythonScriptStep featurization_step created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the pipeline and submit an Experiment run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Pipeline is built\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[featurization_step])\n",
    "print (\"== Pipeline is built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step featurize.py [998da97b][41ef6415-e70e-424d-bd82-3f0897d4a9ff], (This step is eligible to reuse a previous run's output)\n",
      "Submitted PipelineRun 22f3dd5c-6b24-4a7a-b9cf-4eb847a43505\n",
      "Link to Azure Machine Learning studio: https://ml.azure.com/experiments/use-case-2-data-prep/runs/22f3dd5c-6b24-4a7a-b9cf-4eb847a43505?wsid=/subscriptions/c5ec24ce-9c5f-4da2-bf12-9ca8e9758d60/resourcegroups/azure-ml-workshop/workspaces/agd-mlws\n",
      "== Pipeline is submitted for execution\n"
     ]
    }
   ],
   "source": [
    "pipeline_run = Experiment(ws, 'use-case-2-data-prep').submit(pipeline)\n",
    "print(\"== Pipeline is submitted for execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d47047dcba4708992955d006c02c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Running\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/use-case-2-data-prep/runs/22f3dd5c-6b24-4a7a-b9cf-4eb847a43505?wsid=/subscriptions/c5ec24ce-9c5f-4da2-bf12-9ca8e9758d60/resourcegroups/azure-ml-workshop/workspaces/agd-mlws\", \"run_id\": \"22f3dd5c-6b24-4a7a-b9cf-4eb847a43505\", \"run_properties\": {\"run_id\": \"22f3dd5c-6b24-4a7a-b9cf-4eb847a43505\", \"created_utc\": \"2020-04-14T00:11:06.837387Z\", \"properties\": {\"azureml.runsource\": \"azureml.PipelineRun\", \"runSource\": \"SDK\", \"runType\": \"SDK\", \"azureml.parameters\": \"{}\"}, \"tags\": {\"azureml.pipelineComponent\": \"pipelinerun\"}, \"end_time_utc\": null, \"status\": \"Running\", \"log_files\": {\"logs/azureml/executionlogs.txt\": \"https://agdmlws4361999107.blob.core.windows.net/azureml/ExperimentRun/dcid.22f3dd5c-6b24-4a7a-b9cf-4eb847a43505/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=9Vct9RUJGitaKXXOeouhLiucT70x%2BDC0T%2BFPSlDFb%2B8%3D&st=2020-04-14T00%3A06%3A16Z&se=2020-04-14T08%3A16%3A16Z&sp=r\", \"logs/azureml/stderrlogs.txt\": \"https://agdmlws4361999107.blob.core.windows.net/azureml/ExperimentRun/dcid.22f3dd5c-6b24-4a7a-b9cf-4eb847a43505/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=0vWoz4cCEOYHTLH%2FwGvVgszmi1bMYAldLucHI0S5ITs%3D&st=2020-04-14T00%3A06%3A16Z&se=2020-04-14T08%3A16%3A16Z&sp=r\", \"logs/azureml/stdoutlogs.txt\": \"https://agdmlws4361999107.blob.core.windows.net/azureml/ExperimentRun/dcid.22f3dd5c-6b24-4a7a-b9cf-4eb847a43505/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=YKO1OJkEluiqxAXdb5FMwM8t7WvYq2iyQYgzqg2wyb8%3D&st=2020-04-14T00%3A06%3A16Z&se=2020-04-14T08%3A16%3A16Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/executionlogs.txt\", \"logs/azureml/stderrlogs.txt\", \"logs/azureml/stdoutlogs.txt\"]], \"run_duration\": \"0:05:10\"}, \"child_runs\": [{\"run_id\": \"b9c446b8-43b6-4f93-b342-83b501dc0ea3\", \"name\": \"featurize.py\", \"status\": \"Running\", \"start_time\": \"\", \"created_time\": \"2020-04-14T00:11:17.900237Z\", \"end_time\": \"\", \"duration\": \"0:05:00\", \"run_number\": 4, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-04-14T00:11:17.900237Z\", \"is_reused\": \"\"}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2020-04-14 00:11:17Z] Submitting 1 runs, first five are: 998da97b:b9c446b8-43b6-4f93-b342-83b501dc0ea3\\n\", \"graph\": {\"datasource_nodes\": {\"dc197874\": {\"node_id\": \"dc197874\", \"name\": \"clustering-training\"}}, \"module_nodes\": {\"998da97b\": {\"node_id\": \"998da97b\", \"name\": \"featurize.py\", \"status\": \"Running\", \"_is_reused\": false, \"run_id\": \"b9c446b8-43b6-4f93-b342-83b501dc0ea3\"}}, \"edges\": [{\"source_node_id\": \"dc197874\", \"source_node_name\": \"clustering-training\", \"source_name\": \"data\", \"target_name\": \"input\", \"dst_node_id\": \"998da97b\", \"dst_node_name\": \"featurize.py\"}], \"child_runs\": [{\"run_id\": \"b9c446b8-43b6-4f93-b342-83b501dc0ea3\", \"name\": \"featurize.py\", \"status\": \"Running\", \"start_time\": \"\", \"created_time\": \"2020-04-14T00:11:17.900237Z\", \"end_time\": \"\", \"duration\": \"0:05:00\", \"run_number\": 4, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-04-14T00:11:17.900237Z\", \"is_reused\": \"\"}]}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.0.83\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for pipeline run to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineRunId: 22f3dd5c-6b24-4a7a-b9cf-4eb847a43505\n",
      "Link to Portal: https://ml.azure.com/experiments/use-case-2-data-prep/runs/22f3dd5c-6b24-4a7a-b9cf-4eb847a43505?wsid=/subscriptions/c5ec24ce-9c5f-4da2-bf12-9ca8e9758d60/resourcegroups/azure-ml-workshop/workspaces/agd-mlws\n",
      "PipelineRun Status: NotStarted\n",
      "PipelineRun Status: Running\n",
      "\n",
      "\n",
      "StepRunId: b9c446b8-43b6-4f93-b342-83b501dc0ea3\n",
      "Link to Portal: https://ml.azure.com/experiments/use-case-2-data-prep/runs/b9c446b8-43b6-4f93-b342-83b501dc0ea3?wsid=/subscriptions/c5ec24ce-9c5f-4da2-bf12-9ca8e9758d60/resourcegroups/azure-ml-workshop/workspaces/agd-mlws\n",
      "StepRun( featurize.py ) Status: NotStarted\n",
      "StepRun( featurize.py ) Status: Running\n"
     ]
    }
   ],
   "source": [
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See Outputs\n",
    "\n",
    "See where outputs of each pipeline step are located on your datastore.\n",
    "\n",
    "***Wait for pipeline run to complete, to make sure all the outputs are ready***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Steps\n",
    "for step in pipeline_run.get_steps():\n",
    "    print(\"== Outputs of step \" + step.name)\n",
    "    \n",
    "    # Get a dictionary of StepRunOutputs with the output name as the key \n",
    "    output_dict = step.get_outputs()\n",
    "    \n",
    "    for name, output in output_dict.items():\n",
    "        output_reference = output.get_port_data_reference() # Get output port data reference\n",
    "        print(\"\\tname: \" + name)\n",
    "        print(\"\\tdatastore: \" + output_reference.datastore_name)\n",
    "        print(\"\\tpath on datastore: \" + output_reference.path_on_datastore)"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "sanpil"
   }
  ],
  "categories": [
   "how-to-use-azureml",
   "machine-learning-pipelines",
   "intro-to-pipelines"
  ],
  "category": "tutorial",
  "compute": [
   "AML Compute"
  ],
  "datasets": [
   "Custom"
  ],
  "deployment": [
   "None"
  ],
  "exclude_from_index": false,
  "framework": [
   "Azure ML"
  ],
  "friendly_name": "Azure Machine Learning Pipelines with Data Dependency",
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "order_index": 2,
  "star_tag": [
   "featured"
  ],
  "tags": [
   "None"
  ],
  "task": "Demonstrates how to construct a Pipeline with data dependency between steps"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
