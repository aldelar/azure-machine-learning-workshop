{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Machine Learning Pipeline for Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Machine Learning and Pipeline SDK-specific Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.0.83\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "import azureml.dataprep\n",
    "from azureml.core import Workspace, Experiment, Datastore, Dataset\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.data import TabularDataset\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Workspace and Retrieve a Compute Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "create workspace"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Workspace:\n",
      "agd-mlws\n",
      "azure-ml-workshop\n",
      "westus2\n",
      "c5ec24ce-9c5f-4da2-bf12-9ca8e9758d60\n",
      "== Datastore: workspaceblobstore\n",
      "== Compute targets:\n",
      "  agd-inference\n",
      "  agd-inference-v\n",
      "  agd-training-gpu\n",
      "  agd-training-cpu\n",
      "== AML compute target attached: agd-training-cpu\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(\"== Workspace:\")\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n",
    "\n",
    "# Default datastore (Azure blob storage)\n",
    "# def_blob_store = ws.get_default_datastore()\n",
    "blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "print(\"== Datastore: {}\".format(blob_store.name))\n",
    "\n",
    "# list compute targets\n",
    "print(\"== Compute targets:\")\n",
    "for ct in ws.compute_targets:\n",
    "    print(\"  \" + ct)\n",
    "    \n",
    "# Retrieve a compute target    \n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "aml_compute_target = \"agd-training-cpu\"\n",
    "try:\n",
    "    aml_compute = AmlCompute(ws, aml_compute_target)\n",
    "    print(\"== AML compute target attached: \" + aml_compute_target)\n",
    "except ComputeTargetException:\n",
    "    print(\"== AML compute target not found: \" + aml_compute_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Compute Configuration\n",
    "\n",
    "This step uses a docker image, use a [**RunConfiguration**](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.runconfiguration?view=azure-ml-py) to specify these requirements and use when creating the PythonScriptStep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Run Configuration created\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# create a new runconfig object\n",
    "run_config = RunConfiguration()\n",
    "\n",
    "# enable Docker \n",
    "run_config.environment.docker.enabled = True\n",
    "\n",
    "# set Docker base image to the default CPU-based image\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "\n",
    "# use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# specify dependencies\n",
    "#run_config.environment.python.conda_dependencies = CondaDependencies.create(\n",
    "#    conda_packages=['pandas'],\n",
    "#    pip_packages=['azureml-sdk', 'azureml-dataprep[fuse,pandas]', 'azureml-train-automl'], \n",
    "#    pin_sdk_version=False)\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies(\n",
    "    conda_dependencies_file_path='data-prep-pipeline.yml')\n",
    "\n",
    "#\n",
    "print(\"== Run Configuration created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Pipeline Steps with Inputs and Outputs\n",
    "As mentioned earlier, a step in the pipeline can take data as input. This data can be a data source that lives in one of the accessible data locations, or intermediate data produced by a previous step in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines steps\n",
    "Machine learning pipelines can have many steps and these steps could use or reuse datasources and intermediate data. Here's how we construct such a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== PythonScriptStep featurization_step created\n"
     ]
    }
   ],
   "source": [
    "# The best practice is to use separate folders for scripts and its dependent files\n",
    "# for each step and specify that folder as the source_directory for the step.\n",
    "# This helps reduce the size of the snapshot created for the step (only the specific folder is snapshotted).\n",
    "# Since changes in any files in the source_directory would trigger a re-upload of the snapshot, this helps\n",
    "# keep the reuse of the step when there are no changes in the source_directory of the step.\n",
    "source_directory_featurization = 'src/featurization'\n",
    "\n",
    "# Input\n",
    "clustering_ds = Dataset.get_by_name(ws,\"clustering-training\")\n",
    "\n",
    "# Output\n",
    "clustering_featurized_ds = PipelineData(\"output\", datastore=blob_store).as_dataset()\n",
    "clustering_featurized_ds.register(name=\"clustering-training-featurized\", create_new_version=True)\n",
    "\n",
    "# Featurize date/hour columns\n",
    "featurization_step = PythonScriptStep(\n",
    "    script_name=\"featurize.py\", \n",
    "    arguments=[\"--date_column\",\"date\",\n",
    "               \"--hour_column\",\"he\",\n",
    "               \"--datetime_column_name\",\"DATETIME\",\n",
    "               \"--output\", clustering_featurized_ds],\n",
    "    inputs=[clustering_ds.as_named_input(\"input\")],\n",
    "    outputs=[clustering_featurized_ds],\n",
    "    compute_target=aml_compute,\n",
    "    source_directory=source_directory_featurization,\n",
    "    runconfig=run_config\n",
    ")\n",
    "print(\"== PythonScriptStep featurization_step created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the pipeline and submit an Experiment run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Pipeline is built\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[featurization_step])\n",
    "print (\"== Pipeline is built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step featurize.py [998da97b][41ef6415-e70e-424d-bd82-3f0897d4a9ff], (This step is eligible to reuse a previous run's output)\n",
      "Submitted PipelineRun 22f3dd5c-6b24-4a7a-b9cf-4eb847a43505\n",
      "Link to Azure Machine Learning studio: https://ml.azure.com/experiments/use-case-2-data-prep/runs/22f3dd5c-6b24-4a7a-b9cf-4eb847a43505?wsid=/subscriptions/c5ec24ce-9c5f-4da2-bf12-9ca8e9758d60/resourcegroups/azure-ml-workshop/workspaces/agd-mlws\n",
      "== Pipeline is submitted for execution\n"
     ]
    }
   ],
   "source": [
    "pipeline_run = Experiment(ws, 'use-case-2-data-prep').submit(pipeline)\n",
    "print(\"== Pipeline is submitted for execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d47047dcba4708992955d006c02c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Running\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/use-case-2-data-prep/runs/22f3dd5c-6b24-4a7a-b9cf-4eb847a43505?wsid=/subscriptions/c5ec24ce-9c5f-4da2-bf12-9ca8e9758d60/resourcegroups/azure-ml-workshop/workspaces/agd-mlws\", \"run_id\": \"22f3dd5c-6b24-4a7a-b9cf-4eb847a43505\", \"run_properties\": {\"run_id\": \"22f3dd5c-6b24-4a7a-b9cf-4eb847a43505\", \"created_utc\": \"2020-04-14T00:11:06.837387Z\", \"properties\": {\"azureml.runsource\": \"azureml.PipelineRun\", \"runSource\": \"SDK\", \"runType\": \"SDK\", \"azureml.parameters\": \"{}\"}, \"tags\": {\"azureml.pipelineComponent\": \"pipelinerun\"}, \"end_time_utc\": null, \"status\": \"Running\", \"log_files\": {\"logs/azureml/executionlogs.txt\": \"https://agdmlws4361999107.blob.core.windows.net/azureml/ExperimentRun/dcid.22f3dd5c-6b24-4a7a-b9cf-4eb847a43505/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=9Vct9RUJGitaKXXOeouhLiucT70x%2BDC0T%2BFPSlDFb%2B8%3D&st=2020-04-14T00%3A06%3A16Z&se=2020-04-14T08%3A16%3A16Z&sp=r\", \"logs/azureml/stderrlogs.txt\": \"https://agdmlws4361999107.blob.core.windows.net/azureml/ExperimentRun/dcid.22f3dd5c-6b24-4a7a-b9cf-4eb847a43505/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=0vWoz4cCEOYHTLH%2FwGvVgszmi1bMYAldLucHI0S5ITs%3D&st=2020-04-14T00%3A06%3A16Z&se=2020-04-14T08%3A16%3A16Z&sp=r\", \"logs/azureml/stdoutlogs.txt\": \"https://agdmlws4361999107.blob.core.windows.net/azureml/ExperimentRun/dcid.22f3dd5c-6b24-4a7a-b9cf-4eb847a43505/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=YKO1OJkEluiqxAXdb5FMwM8t7WvYq2iyQYgzqg2wyb8%3D&st=2020-04-14T00%3A06%3A16Z&se=2020-04-14T08%3A16%3A16Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/executionlogs.txt\", \"logs/azureml/stderrlogs.txt\", \"logs/azureml/stdoutlogs.txt\"]], \"run_duration\": \"0:05:10\"}, \"child_runs\": [{\"run_id\": \"b9c446b8-43b6-4f93-b342-83b501dc0ea3\", \"name\": \"featurize.py\", \"status\": \"Running\", \"start_time\": \"\", \"created_time\": \"2020-04-14T00:11:17.900237Z\", \"end_time\": \"\", \"duration\": \"0:05:00\", \"run_number\": 4, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-04-14T00:11:17.900237Z\", \"is_reused\": \"\"}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2020-04-14 00:11:17Z] Submitting 1 runs, first five are: 998da97b:b9c446b8-43b6-4f93-b342-83b501dc0ea3\\n\", \"graph\": {\"datasource_nodes\": {\"dc197874\": {\"node_id\": \"dc197874\", \"name\": \"clustering-training\"}}, \"module_nodes\": {\"998da97b\": {\"node_id\": \"998da97b\", \"name\": \"featurize.py\", \"status\": \"Running\", \"_is_reused\": false, \"run_id\": \"b9c446b8-43b6-4f93-b342-83b501dc0ea3\"}}, \"edges\": [{\"source_node_id\": \"dc197874\", \"source_node_name\": \"clustering-training\", \"source_name\": \"data\", \"target_name\": \"input\", \"dst_node_id\": \"998da97b\", \"dst_node_name\": \"featurize.py\"}], \"child_runs\": [{\"run_id\": \"b9c446b8-43b6-4f93-b342-83b501dc0ea3\", \"name\": \"featurize.py\", \"status\": \"Running\", \"start_time\": \"\", \"created_time\": \"2020-04-14T00:11:17.900237Z\", \"end_time\": \"\", \"duration\": \"0:05:00\", \"run_number\": 4, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-04-14T00:11:17.900237Z\", \"is_reused\": \"\"}]}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.0.83\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for pipeline run to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineRunId: 22f3dd5c-6b24-4a7a-b9cf-4eb847a43505\n",
      "Link to Portal: https://ml.azure.com/experiments/use-case-2-data-prep/runs/22f3dd5c-6b24-4a7a-b9cf-4eb847a43505?wsid=/subscriptions/c5ec24ce-9c5f-4da2-bf12-9ca8e9758d60/resourcegroups/azure-ml-workshop/workspaces/agd-mlws\n",
      "PipelineRun Status: NotStarted\n",
      "PipelineRun Status: Running\n",
      "\n",
      "\n",
      "StepRunId: b9c446b8-43b6-4f93-b342-83b501dc0ea3\n",
      "Link to Portal: https://ml.azure.com/experiments/use-case-2-data-prep/runs/b9c446b8-43b6-4f93-b342-83b501dc0ea3?wsid=/subscriptions/c5ec24ce-9c5f-4da2-bf12-9ca8e9758d60/resourcegroups/azure-ml-workshop/workspaces/agd-mlws\n",
      "StepRun( featurize.py ) Status: NotStarted\n",
      "StepRun( featurize.py ) Status: Running\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_f91aff41b10e6bc3751c8c01038f29ba5b6ce43fc32986864a3653370426d186_d.txt\n",
      "========================================================================================================================\n",
      "2020-04-14T00:26:34Z Starting output-watcher...\n",
      "2020-04-14T00:26:34Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_fbecfa6255cba4c1cc15571eceadee24\n",
      "Digest: sha256:557c13e8b0d913a64063b103e8bd03f0268c60bb46570dc191a4902c2f6e09b2\n",
      "Status: Image is up to date for agdmlwsb20e062b.azurecr.io/azureml/azureml_fbecfa6255cba4c1cc15571eceadee24:latest\n",
      "0fbcf87f98380dc87fcb88917ec7123e8bf632a9f3cdab458a05588dab71c063\n",
      "2020/04/14 00:26:37 Version: 3.0.01172.0001 Branch: master Commit: d33e301a\n",
      "2020/04/14 00:26:37 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      "2020/04/14 00:26:37 sshd runtime has already been installed in the container\n",
      "\n",
      "Streaming azureml-logs/65_job_prep-tvmps_f91aff41b10e6bc3751c8c01038f29ba5b6ce43fc32986864a3653370426d186_d.txt\n",
      "===============================================================================================================\n",
      "Starting job_prep.py script\n",
      "Starting job preparation. Current time:2020-04-14T00:26:46.782172\n",
      "Extracting the control code.\n",
      "fetching and extracting the control code on master node.\n",
      "Retrieving project from snapshot: 0a51b897-7d54-4c9b-a884-e7259d349760\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 86\n",
      "Starting project file download.\n",
      "Finished project file download.\n",
      "Download from datastores if requested.\n",
      "Acquired lockfile /tmp/b9c446b8-43b6-4f93-b342-83b501dc0ea3-datastore.lock to downloading input data references\n",
      "Download or mount from datasets if requested.\n",
      "Job preparation is complete. Current time:2020-04-14T00:26:51.017192\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 144\n",
      "Entering Run History Context Manager.\n",
      "Preparing to call script [ featurize.py ] with arguments: ['--date_column', 'date', '--hour_column', 'he', '--datetime_column_name', 'DATETIME', '--output', '/mnt/batch/tasks/shared/LS_root/jobs/agd-mlws/azureml/b9c446b8-43b6-4f93-b342-83b501dc0ea3/mounts/workspaceblobstore/azureml/b9c446b8-43b6-4f93-b342-83b501dc0ea3/output']\n",
      "After variable expansion, calling script [ featurize.py ] with arguments: ['--date_column', 'date', '--hour_column', 'he', '--datetime_column_name', 'DATETIME', '--output', '/mnt/batch/tasks/shared/LS_root/jobs/agd-mlws/azureml/b9c446b8-43b6-4f93-b342-83b501dc0ea3/mounts/workspaceblobstore/azureml/b9c446b8-43b6-4f93-b342-83b501dc0ea3/output']\n",
      "\n",
      "Date Column: date\n",
      "Hour Column: he\n",
      "Datetime Column Name: DATETIME\n",
      "Output Data: /mnt/batch/tasks/shared/LS_root/jobs/agd-mlws/azureml/b9c446b8-43b6-4f93-b342-83b501dc0ea3/mounts/workspaceblobstore/azureml/b9c446b8-43b6-4f93-b342-83b501dc0ea3/output\n",
      "\n",
      "\n",
      "The experiment failed. Finalizing run...\n",
      "Cleaning up all outstanding Run operations, waiting 300.0 seconds\n",
      "2 items cleaning up...\n",
      "Cleanup took 0.4305703639984131 seconds\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 144\n",
      "Traceback (most recent call last):\n",
      "  File \"featurize.py\", line 31, in <module>\n",
      "    input_df = input_ds.to_pandas_dataframe()\n",
      "  File \"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/site-packages/azureml/data/_loggerfactory.py\", line 106, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/site-packages/azureml/data/tabular_dataset.py\", line 166, in to_pandas_dataframe\n",
      "    df = _try_execute(lambda: dataflow.to_pandas_dataframe(on_error=on_error,\n",
      "  File \"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/site-packages/azureml/data/dataset_error_handling.py\", line 85, in _try_execute\n",
      "    raise DatasetExecutionError(str(e))\n",
      "azureml.data.dataset_error_handling.DatasetExecutionError: (Reading 30809 bytes from azure blob storage timed out after 38814.5256 ms.)|session_id=0f4fe872-f748-4418-88a0-e1b155609845\n",
      "\n",
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_f91aff41b10e6bc3751c8c01038f29ba5b6ce43fc32986864a3653370426d186_d.txt\n",
      "===============================================================================================================\n",
      "Starting job release. Current time:2020-04-14T00:51:39.480338\n",
      "Logging experiment finalizing status in history service.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 678\n",
      "Job release is complete. Current time:2020-04-14T00:51:42.405331\n",
      "\n",
      "StepRun(featurize.py) Execution Summary\n",
      "========================================\n",
      "StepRun( featurize.py ) Status: Failed\n",
      "\n",
      "Warnings:\n",
      "{\n",
      "  \"error\": {\n",
      "    \"code\": \"UserError\",\n",
      "    \"message\": \"AzureMLCompute job failed.\\nJobFailed: Submitted script failed with a non-zero exit code; see the driver log file for details.\",\n",
      "    \"messageFormat\": null,\n",
      "    \"messageParameters\": {},\n",
      "    \"referenceCode\": null,\n",
      "    \"detailsUri\": null,\n",
      "    \"target\": null,\n",
      "    \"details\": [],\n",
      "    \"innerError\": null,\n",
      "    \"debugInfo\": null\n",
      "  },\n",
      "  \"correlation\": {\n",
      "    \"operation\": null,\n",
      "    \"request\": \"d23157592ca1e30d\"\n",
      "  },\n",
      "  \"environment\": \"westus2\",\n",
      "  \"location\": \"westus2\",\n",
      "  \"time\": \"2020-04-14T00:52:01.4202117+00:00\"\n",
      "}\n"
     ]
    },
    {
     "ename": "ActivityFailedException",
     "evalue": "ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"User program failed with DatasetExecutionError: (Reading 30809 bytes from azure blob storage timed out after 38814.5256 ms.)|session_id=0f4fe872-f748-4418-88a0-e1b155609845\",\n        \"detailsUri\": \"https://aka.ms/azureml-known-errors\",\n        \"details\": [],\n        \"debugInfo\": {\n            \"type\": \"DatasetExecutionError\",\n            \"message\": \"(Reading 30809 bytes from azure blob storage timed out after 38814.5256 ms.)|session_id=0f4fe872-f748-4418-88a0-e1b155609845\",\n            \"stackTrace\": \"  File \\\"/mnt/batch/tasks/shared/LS_root/jobs/agd-mlws/azureml/b9c446b8-43b6-4f93-b342-83b501dc0ea3/mounts/workspaceblobstore/azureml/b9c446b8-43b6-4f93-b342-83b501dc0ea3/azureml-setup/context_manager_injector.py\\\", line 127, in execute_with_context\\n    runpy.run_path(sys.argv[0], globals(), run_name=\\\"__main__\\\")\\n  File \\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/runpy.py\\\", line 263, in run_path\\n    pkg_name=pkg_name, script_name=fname)\\n  File \\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/runpy.py\\\", line 96, in _run_module_code\\n    mod_name, mod_spec, pkg_name, script_name)\\n  File \\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/runpy.py\\\", line 85, in _run_code\\n    exec(code, run_globals)\\n  File \\\"featurize.py\\\", line 31, in <module>\\n    input_df = input_ds.to_pandas_dataframe()\\n  File \\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/site-packages/azureml/data/_loggerfactory.py\\\", line 106, in wrapper\\n    return func(*args, **kwargs)\\n  File \\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/site-packages/azureml/data/tabular_dataset.py\\\", line 166, in to_pandas_dataframe\\n    df = _try_execute(lambda: dataflow.to_pandas_dataframe(on_error=on_error,\\n  File \\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/site-packages/azureml/data/dataset_error_handling.py\\\", line 85, in _try_execute\\n    raise DatasetExecutionError(str(e))\\n\"\n        }\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"User program failed with DatasetExecutionError: (Reading 30809 bytes from azure blob storage timed out after 38814.5256 ms.)|session_id=0f4fe872-f748-4418-88a0-e1b155609845\\\",\\n        \\\"detailsUri\\\": \\\"https://aka.ms/azureml-known-errors\\\",\\n        \\\"details\\\": [],\\n        \\\"debugInfo\\\": {\\n            \\\"type\\\": \\\"DatasetExecutionError\\\",\\n            \\\"message\\\": \\\"(Reading 30809 bytes from azure blob storage timed out after 38814.5256 ms.)|session_id=0f4fe872-f748-4418-88a0-e1b155609845\\\",\\n            \\\"stackTrace\\\": \\\"  File \\\\\\\"/mnt/batch/tasks/shared/LS_root/jobs/agd-mlws/azureml/b9c446b8-43b6-4f93-b342-83b501dc0ea3/mounts/workspaceblobstore/azureml/b9c446b8-43b6-4f93-b342-83b501dc0ea3/azureml-setup/context_manager_injector.py\\\\\\\", line 127, in execute_with_context\\\\n    runpy.run_path(sys.argv[0], globals(), run_name=\\\\\\\"__main__\\\\\\\")\\\\n  File \\\\\\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/runpy.py\\\\\\\", line 263, in run_path\\\\n    pkg_name=pkg_name, script_name=fname)\\\\n  File \\\\\\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/runpy.py\\\\\\\", line 96, in _run_module_code\\\\n    mod_name, mod_spec, pkg_name, script_name)\\\\n  File \\\\\\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/runpy.py\\\\\\\", line 85, in _run_code\\\\n    exec(code, run_globals)\\\\n  File \\\\\\\"featurize.py\\\\\\\", line 31, in <module>\\\\n    input_df = input_ds.to_pandas_dataframe()\\\\n  File \\\\\\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/site-packages/azureml/data/_loggerfactory.py\\\\\\\", line 106, in wrapper\\\\n    return func(*args, **kwargs)\\\\n  File \\\\\\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/site-packages/azureml/data/tabular_dataset.py\\\\\\\", line 166, in to_pandas_dataframe\\\\n    df = _try_execute(lambda: dataflow.to_pandas_dataframe(on_error=on_error,\\\\n  File \\\\\\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/site-packages/azureml/data/dataset_error_handling.py\\\\\\\", line 85, in _try_execute\\\\n    raise DatasetExecutionError(str(e))\\\\n\\\"\\n        }\\n    },\\n    \\\"time\\\": \\\"0001-01-01T00:00:00.000Z\\\"\\n}\"\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mActivityFailedException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-04685269429d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/pipeline/core/run.py\u001b[0m in \u001b[0;36mwait_for_completion\u001b[0;34m(self, show_output, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    289\u001b[0m                             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                             step_run.wait_for_completion(timeout_seconds=timeout_seconds - time_elapsed,\n\u001b[0;32m--> 291\u001b[0;31m                                                          raise_on_error=raise_on_error)\n\u001b[0m\u001b[1;32m    292\u001b[0m                             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtimeout_seconds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/pipeline/core/run.py\u001b[0m in \u001b[0;36mwait_for_completion\u001b[0;34m(self, show_output, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m                 return self._stream_run_output(timeout_seconds=timeout_seconds,\n\u001b[0;32m--> 716\u001b[0;31m                                                raise_on_error=raise_on_error)\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                 \u001b[0merror_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The output streaming for the run interrupted.\\n\"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/pipeline/core/run.py\u001b[0m in \u001b[0;36m_stream_run_output\u001b[0;34m(self, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mraise_on_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mActivityFailedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_details\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_details\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mActivityFailedException\u001b[0m: ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"User program failed with DatasetExecutionError: (Reading 30809 bytes from azure blob storage timed out after 38814.5256 ms.)|session_id=0f4fe872-f748-4418-88a0-e1b155609845\",\n        \"detailsUri\": \"https://aka.ms/azureml-known-errors\",\n        \"details\": [],\n        \"debugInfo\": {\n            \"type\": \"DatasetExecutionError\",\n            \"message\": \"(Reading 30809 bytes from azure blob storage timed out after 38814.5256 ms.)|session_id=0f4fe872-f748-4418-88a0-e1b155609845\",\n            \"stackTrace\": \"  File \\\"/mnt/batch/tasks/shared/LS_root/jobs/agd-mlws/azureml/b9c446b8-43b6-4f93-b342-83b501dc0ea3/mounts/workspaceblobstore/azureml/b9c446b8-43b6-4f93-b342-83b501dc0ea3/azureml-setup/context_manager_injector.py\\\", line 127, in execute_with_context\\n    runpy.run_path(sys.argv[0], globals(), run_name=\\\"__main__\\\")\\n  File \\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/runpy.py\\\", line 263, in run_path\\n    pkg_name=pkg_name, script_name=fname)\\n  File \\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/runpy.py\\\", line 96, in _run_module_code\\n    mod_name, mod_spec, pkg_name, script_name)\\n  File \\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/runpy.py\\\", line 85, in _run_code\\n    exec(code, run_globals)\\n  File \\\"featurize.py\\\", line 31, in <module>\\n    input_df = input_ds.to_pandas_dataframe()\\n  File \\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/site-packages/azureml/data/_loggerfactory.py\\\", line 106, in wrapper\\n    return func(*args, **kwargs)\\n  File \\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/site-packages/azureml/data/tabular_dataset.py\\\", line 166, in to_pandas_dataframe\\n    df = _try_execute(lambda: dataflow.to_pandas_dataframe(on_error=on_error,\\n  File \\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/site-packages/azureml/data/dataset_error_handling.py\\\", line 85, in _try_execute\\n    raise DatasetExecutionError(str(e))\\n\"\n        }\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"User program failed with DatasetExecutionError: (Reading 30809 bytes from azure blob storage timed out after 38814.5256 ms.)|session_id=0f4fe872-f748-4418-88a0-e1b155609845\\\",\\n        \\\"detailsUri\\\": \\\"https://aka.ms/azureml-known-errors\\\",\\n        \\\"details\\\": [],\\n        \\\"debugInfo\\\": {\\n            \\\"type\\\": \\\"DatasetExecutionError\\\",\\n            \\\"message\\\": \\\"(Reading 30809 bytes from azure blob storage timed out after 38814.5256 ms.)|session_id=0f4fe872-f748-4418-88a0-e1b155609845\\\",\\n            \\\"stackTrace\\\": \\\"  File \\\\\\\"/mnt/batch/tasks/shared/LS_root/jobs/agd-mlws/azureml/b9c446b8-43b6-4f93-b342-83b501dc0ea3/mounts/workspaceblobstore/azureml/b9c446b8-43b6-4f93-b342-83b501dc0ea3/azureml-setup/context_manager_injector.py\\\\\\\", line 127, in execute_with_context\\\\n    runpy.run_path(sys.argv[0], globals(), run_name=\\\\\\\"__main__\\\\\\\")\\\\n  File \\\\\\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/runpy.py\\\\\\\", line 263, in run_path\\\\n    pkg_name=pkg_name, script_name=fname)\\\\n  File \\\\\\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/runpy.py\\\\\\\", line 96, in _run_module_code\\\\n    mod_name, mod_spec, pkg_name, script_name)\\\\n  File \\\\\\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/runpy.py\\\\\\\", line 85, in _run_code\\\\n    exec(code, run_globals)\\\\n  File \\\\\\\"featurize.py\\\\\\\", line 31, in <module>\\\\n    input_df = input_ds.to_pandas_dataframe()\\\\n  File \\\\\\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/site-packages/azureml/data/_loggerfactory.py\\\\\\\", line 106, in wrapper\\\\n    return func(*args, **kwargs)\\\\n  File \\\\\\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/site-packages/azureml/data/tabular_dataset.py\\\\\\\", line 166, in to_pandas_dataframe\\\\n    df = _try_execute(lambda: dataflow.to_pandas_dataframe(on_error=on_error,\\\\n  File \\\\\\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/site-packages/azureml/data/dataset_error_handling.py\\\\\\\", line 85, in _try_execute\\\\n    raise DatasetExecutionError(str(e))\\\\n\\\"\\n        }\\n    },\\n    \\\"time\\\": \\\"0001-01-01T00:00:00.000Z\\\"\\n}\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See Outputs\n",
    "\n",
    "See where outputs of each pipeline step are located on your datastore.\n",
    "\n",
    "***Wait for pipeline run to complete, to make sure all the outputs are ready***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Steps\n",
    "for step in pipeline_run.get_steps():\n",
    "    print(\"== Outputs of step \" + step.name)\n",
    "    \n",
    "    # Get a dictionary of StepRunOutputs with the output name as the key \n",
    "    output_dict = step.get_outputs()\n",
    "    \n",
    "    for name, output in output_dict.items():\n",
    "        output_reference = output.get_port_data_reference() # Get output port data reference\n",
    "        print(\"\\tname: \" + name)\n",
    "        print(\"\\tdatastore: \" + output_reference.datastore_name)\n",
    "        print(\"\\tpath on datastore: \" + output_reference.path_on_datastore)"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "sanpil"
   }
  ],
  "categories": [
   "how-to-use-azureml",
   "machine-learning-pipelines",
   "intro-to-pipelines"
  ],
  "category": "tutorial",
  "compute": [
   "AML Compute"
  ],
  "datasets": [
   "Custom"
  ],
  "deployment": [
   "None"
  ],
  "exclude_from_index": false,
  "framework": [
   "Azure ML"
  ],
  "friendly_name": "Azure Machine Learning Pipelines with Data Dependency",
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "order_index": 2,
  "star_tag": [
   "featured"
  ],
  "tags": [
   "None"
  ],
  "task": "Demonstrates how to construct a Pipeline with data dependency between steps"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
