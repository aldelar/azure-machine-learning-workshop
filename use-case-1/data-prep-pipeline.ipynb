{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Machine Learning Pipelines with Data Dependency\n",
    "In this notebook, we will see how we can build a pipeline with implicit data dependency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Machine Learning and Pipeline SDK-specific Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Datastore, Dataset\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.data import TabularDataset\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Workspace and Retrieve a Compute Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create workspace"
    ]
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(\"== Workspace:\")\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n",
    "\n",
    "# Default datastore (Azure blob storage)\n",
    "# def_blob_store = ws.get_default_datastore()\n",
    "blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "print(\"== Datastore: {}\".format(blob_store.name))\n",
    "\n",
    "# list compute targets\n",
    "print(\"== Compute targets:\")\n",
    "for ct in ws.compute_targets:\n",
    "    print(\"  \" + ct)\n",
    "    \n",
    "# Retrieve a compute target    \n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "aml_compute_target = \"agd-training-gpu\"\n",
    "try:\n",
    "    aml_compute = AmlCompute(ws, aml_compute_target)\n",
    "    print(\"== AML compute target attached: \" + aml_compute_target)\n",
    "except ComputeTargetException:\n",
    "    print(\"== AML compute target not found: \" + aml_compute_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Compute Configuration\n",
    "\n",
    "This step uses a docker image, use a [**RunConfiguration**](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.runconfiguration?view=azure-ml-py) to specify these requirements and use when creating the PythonScriptStep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# create a new runconfig object\n",
    "run_config = RunConfiguration()\n",
    "\n",
    "# enable Docker \n",
    "run_config.environment.docker.enabled = False\n",
    "\n",
    "# set Docker base image to the default CPU-based image\n",
    "#run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "\n",
    "# use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# specify dependencies\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(\n",
    "    pip_packages=['azureml-dataprep[fuse,pandas]'])\n",
    "    #    conda_packages=['pandas'],\n",
    "#    pip_packages=['azureml-sdk', 'azureml-dataprep', 'azureml-train-automl'], \n",
    "#    pin_sdk_version=False)\n",
    "#run_config.environment.python.conda_dependencies = CondaDependencies(\n",
    "#    conda_dependencies_file_path='data-prep-pipeline.yml')\n",
    "\n",
    "#\n",
    "print(\"== Run Configuration created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Pipeline Steps with Inputs and Outputs\n",
    "As mentioned earlier, a step in the pipeline can take data as input. This data can be a data source that lives in one of the accessible data locations, or intermediate data produced by a previous step in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasources as Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hourly time series\n",
    "h_time_series_1_ds = Dataset.get_by_name(ws,\"h_time_series_1\")\n",
    "h_time_series_2_ds = Dataset.get_by_name(ws,\"h_time_series_2\")\n",
    "h_time_series_3_ds = Dataset.get_by_name(ws,\"h_time_series_3\")\n",
    "# daily time series\n",
    "d_time_series_1_dr = DataReference(datastore=blob_store,\n",
    "                                   data_reference_name=\"d_time_series_1\",\n",
    "                                   path_on_datastore=\"datasets/time-series/X1.csv\")\n",
    "d_time_series_2_dr = DataReference(datastore=blob_store,\n",
    "                                   data_reference_name=\"d_time_series_2\",\n",
    "                                   path_on_datastore=\"datasets/time-series/X2.csv\")\n",
    "\n",
    "print(\"== Datasets metadata retrieved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermediate/Output Data\n",
    "Intermediate data (or output of a Step) is represented by **[PipelineData](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinedata?view=azure-ml-py)** object. PipelineData can be produced by one step and consumed in another step by providing the PipelineData object as an output of one step and the input of one or more steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define intermediate data using PipelineData\n",
    "# hourly series that need to have timestampts generated from columns and pivoted if necessary\n",
    "h_time_series_1_pivot_pd = PipelineData(\"h_time_series_1_pivot\",datastore=blob_store)\n",
    "h_time_series_2_pivot_pd = PipelineData(\"h_time_series_2_pivot\",datastore=blob_store)\n",
    "h_time_series_3_pivot_pd = PipelineData(\"h_time_series_3_pivot\",datastore=blob_store)\n",
    "print(\"== Intermediate PipelineData references created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines steps using datasources and intermediate data\n",
    "Machine learning pipelines can have many steps and these steps could use or reuse datasources and intermediate data. Here's how we construct such a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best practice is to use separate folders for scripts and its dependent files\n",
    "# for each step and specify that folder as the source_directory for the step.\n",
    "# This helps reduce the size of the snapshot created for the step (only the specific folder is snapshotted).\n",
    "# Since changes in any files in the source_directory would trigger a re-upload of the snapshot, this helps\n",
    "# keep the reuse of the step when there are no changes in the source_directory of the step.\n",
    "source_directory_pivot = 'src/pivot'\n",
    "source_directory_join = 'src/join'\n",
    "source_directory_groupby_aggregate = 'src/groupby-aggregate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step4 consumes the datasource (Datareference) in the previous step\n",
    "# and produces processed_data1\n",
    "h_time_series_1_pivot_step = PythonScriptStep(\n",
    "    script_name=\"pivot.py\", \n",
    "    arguments=[\"--date_column\",\"MYDATE\",\n",
    "               \"--hour_column\",\"HOUR\",\n",
    "               \"--datetime_column_name\",\"DATETIME\",\n",
    "               \"--pivot_columns\",\"NODE_ID\",\n",
    "               \"--value_column\",\"MW\",\n",
    "               \"--output\", h_time_series_1_pivot_pd],\n",
    "    inputs=[h_time_series_1_ds.as_named_input(\"time_series\")],\n",
    "    outputs=[h_time_series_1_pivot_pd],\n",
    "    compute_target=aml_compute, \n",
    "    source_directory=source_directory_pivot,\n",
    "    runconfig=run_config\n",
    ")\n",
    "print(\"== PythonScriptStep time_series_1_pivot_step created\")\n",
    "\n",
    "h_time_series_2_pivot_step = PythonScriptStep(\n",
    "    script_name=\"pivot.py\", \n",
    "    arguments=[\"--date_column\",\"MYDATE\",\n",
    "               \"--hour_column\",\"HOUR\",\n",
    "               \"--datetime_column_name\",\"DATETIME\",\n",
    "               \"--pivot_columns\",\"NODE_ID\",\n",
    "               \"--value_column\",\"MW\",\n",
    "               \"--output\", h_time_series_2_pivot_pd],\n",
    "    inputs=[h_time_series_2_ds.as_named_input(\"time_series\")],\n",
    "    outputs=[h_time_series_2_pivot_pd],\n",
    "    compute_target=aml_compute, \n",
    "    source_directory=source_directory_pivot,\n",
    "    runconfig=run_config\n",
    ")\n",
    "print(\"== PythonScriptStep time_series_2_pivot_step created\")\n",
    "\n",
    "h_time_series_3_pivot_step = PythonScriptStep(\n",
    "    script_name=\"pivot.py\", \n",
    "    arguments=[\"--date_column\",\"DATE\",\n",
    "               \"--hour_column\",\"HE\",\n",
    "               \"--datetime_column_name\",\"DATETIME\",\n",
    "               \"--pivot_columns\",\"\",\n",
    "               \"--value_column\",\"CLOAD\",\n",
    "               \"--output\", h_time_series_3_pivot_pd],\n",
    "    inputs=[h_time_series_3_ds.as_named_input(\"time_series\")],\n",
    "    outputs=[h_time_series_3_pivot_pd],\n",
    "    compute_target=aml_compute, \n",
    "    source_directory=source_directory_pivot,\n",
    "    runconfig=run_config\n",
    ")\n",
    "print(\"== PythonScriptStep time_series_3_pivot_step created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a Step that consumes intermediate data and produces intermediate data\n",
    "In this step, we define a step that consumes an intermediate data and produces intermediate data.\n",
    "\n",
    "**Open `join.py` in the local machine and examine the arguments, inputs, and outputs for the script. That will give you a good sense of why the script argument names used below are important.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining all HOURLY pivoted time-series\n",
    "h_time_series_joined_ds = PipelineData('h_time_series_joined', datastore=blob_store).as_dataset()\n",
    "h_time_series_joined_ds = h_time_series_joined_ds.register(name='h_time_series_joined', create_new_version=True)\n",
    "\n",
    "# Join Step\n",
    "h_join_step = PythonScriptStep(\n",
    "    script_name=\"join.py\",\n",
    "    arguments=[\"--join_column\", \"DATETIME\",\n",
    "               \"--input_1\", h_time_series_1_pivot_pd,\n",
    "               \"--input_2\", h_time_series_2_pivot_pd,\n",
    "               \"--input_3\", h_time_series_3_pivot_pd,\n",
    "               \"--output\", h_time_series_joined_ds],\n",
    "    inputs=[h_time_series_1_pivot_pd,\n",
    "            h_time_series_2_pivot_pd,\n",
    "            h_time_series_3_pivot_pd],\n",
    "    outputs=[h_time_series_joined_ds],\n",
    "    compute_target=aml_compute, \n",
    "    source_directory=source_directory_join)\n",
    "\n",
    "print(\"== PythonScriptStep h_join_step created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining all DAILY time-series\n",
    "d_time_series_joined_ds = PipelineData('d_time_series_joined', datastore=blob_store).as_dataset()\n",
    "d_time_series_joined_ds = d_time_series_joined_ds.register(name='d_time_series_joined', create_new_version=True)\n",
    "\n",
    "# Join Step\n",
    "d_join_step = PythonScriptStep(\n",
    "    script_name=\"join.py\",\n",
    "    arguments=[\"--join_column\", \"RDATE\",\n",
    "               \"--input_1\", d_time_series_1_dr,\n",
    "               \"--input_2\", d_time_series_2_dr,\n",
    "               \"--output\", d_time_series_joined_ds],\n",
    "    inputs=[d_time_series_1_dr,\n",
    "            d_time_series_2_dr],\n",
    "    outputs=[d_time_series_joined_ds],\n",
    "    compute_target=aml_compute,\n",
    "    source_directory=source_directory_join)\n",
    "\n",
    "print(\"== PythonScriptStep d_join_step created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate stats for HOURLY joined series GROUP BY DAILY\n",
    "h_time_series_joined_groupby_aggregated_ds = PipelineData('h_time_series_joined_groupby_aggregated', datastore=blob_store).as_dataset()\n",
    "h_time_series_joined_groupby_aggregated_ds = h_time_series_joined_groupby_aggregated_ds.register(name='h_time_series_joined_groupby_aggregated', create_new_version=True)\n",
    "\n",
    "# groupby-aggregate step\n",
    "groupby_aggregate_step = PythonScriptStep(\n",
    "    script_name=\"groupby-aggregate.py\",\n",
    "    arguments=[\"--datetime_column\", \"DATETIME\",\n",
    "               \"--date_column_name\",\"RDATE\",\n",
    "               \"--input_ds\", h_time_series_joined_ds,\n",
    "               \"--output_ds\", h_time_series_joined_groupby_aggregated_ds],\n",
    "    inputs=[h_time_series_joined_ds],\n",
    "    outputs=[h_time_series_joined_groupby_aggregated_ds],\n",
    "    compute_target=aml_compute,\n",
    "    source_directory=source_directory_groupby_aggregate)\n",
    "\n",
    "print(\"== PythonScriptStep groupby_aggregate_step created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the pipeline and submit an Experiment run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[h_join_step,d_join_step,groupby_aggregate_step])\n",
    "print (\"== Pipeline is built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = Experiment(ws, 'use-case-1-data-prep').submit(pipeline)\n",
    "print(\"== Pipeline is submitted for execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for pipeline run to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See Outputs\n",
    "\n",
    "See where outputs of each pipeline step are located on your datastore.\n",
    "\n",
    "***Wait for pipeline run to complete, to make sure all the outputs are ready***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Steps\n",
    "for step in pipeline_run.get_steps():\n",
    "    print(\"== Outputs of step \" + step.name)\n",
    "    \n",
    "    # Get a dictionary of StepRunOutputs with the output name as the key \n",
    "    output_dict = step.get_outputs()\n",
    "    \n",
    "    for name, output in output_dict.items():\n",
    "        output_reference = output.get_port_data_reference() # Get output port data reference\n",
    "        print(\"\\tname: \" + name)\n",
    "        print(\"\\tdatastore: \" + output_reference.datastore_name)\n",
    "        print(\"\\tpath on datastore: \" + output_reference.path_on_datastore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next: Publishing the Pipeline and calling it from the REST endpoint\n",
    "See this [notebook](https://aka.ms/pl-pub-rep) to understand how the pipeline is published and you can call the REST endpoint to run the pipeline."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "sanpil"
   }
  ],
  "categories": [
   "how-to-use-azureml",
   "machine-learning-pipelines",
   "intro-to-pipelines"
  ],
  "category": "tutorial",
  "compute": [
   "AML Compute"
  ],
  "datasets": [
   "Custom"
  ],
  "deployment": [
   "None"
  ],
  "exclude_from_index": false,
  "framework": [
   "Azure ML"
  ],
  "friendly_name": "Azure Machine Learning Pipelines with Data Dependency",
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "order_index": 2,
  "star_tag": [
   "featured"
  ],
  "tags": [
   "None"
  ],
  "task": "Demonstrates how to construct a Pipeline with data dependency between steps"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
