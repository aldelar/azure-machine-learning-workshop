{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Machine Learning Pipelines with Data Dependency\n",
    "In this notebook, we will see how we can build a pipeline with implicit data dependency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Machine Learning and Pipeline SDK-specific Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.0.85\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "import azureml.dataprep\n",
    "from azureml.core import Workspace, Experiment, Datastore, Dataset\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.data import TabularDataset\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Workspace and Retrieve a Compute Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "create workspace"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Workspace:\n",
      "agd-ml-demo\n",
      "azure-ml-workshop\n",
      "westus2\n",
      "c5ec24ce-9c5f-4da2-bf12-9ca8e9758d60\n",
      "== Datastore: workspaceblobstore\n",
      "== Compute targets:\n",
      "  agd-training-cpu\n",
      "== AML compute target attached: agd-training-cpu\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(\"== Workspace:\")\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n",
    "\n",
    "# Default datastore (Azure blob storage)\n",
    "# def_blob_store = ws.get_default_datastore()\n",
    "blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "print(\"== Datastore: {}\".format(blob_store.name))\n",
    "\n",
    "# list compute targets\n",
    "print(\"== Compute targets:\")\n",
    "for ct in ws.compute_targets:\n",
    "    print(\"  \" + ct)\n",
    "    \n",
    "# Retrieve a compute target    \n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "aml_compute_target = \"agd-training-cpu\"\n",
    "try:\n",
    "    aml_compute = AmlCompute(ws, aml_compute_target)\n",
    "    print(\"== AML compute target attached: \" + aml_compute_target)\n",
    "except ComputeTargetException:\n",
    "    print(\"== AML compute target not found: \" + aml_compute_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Compute Configuration\n",
    "\n",
    "This step uses a docker image, use a [**RunConfiguration**](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.runconfiguration?view=azure-ml-py) to specify these requirements and use when creating the PythonScriptStep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Run Configuration created\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# create a new runconfig object\n",
    "run_config = RunConfiguration()\n",
    "\n",
    "# enable Docker \n",
    "run_config.environment.docker.enabled = True\n",
    "\n",
    "# set Docker base image to the default CPU-based image\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "\n",
    "# use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# specify dependencies\n",
    "#run_config.environment.python.conda_dependencies = CondaDependencies.create(\n",
    "#    pip_packages=['azureml-dataprep[fuse,pandas]'])\n",
    "    #    conda_packages=['pandas'],\n",
    "#    pip_packages=['azureml-sdk', 'azureml-dataprep', 'azureml-train-automl'], \n",
    "#    pin_sdk_version=False)\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies(\n",
    "    conda_dependencies_file_path='data-prep-pipeline.yml')\n",
    "\n",
    "#\n",
    "print(\"== Run Configuration created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Pipeline Steps with Inputs and Outputs\n",
    "As mentioned earlier, a step in the pipeline can take data as input. This data can be a data source that lives in one of the accessible data locations, or intermediate data produced by a previous step in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasources as Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Datasets metadata retrieved\n"
     ]
    }
   ],
   "source": [
    "# hourly time series\n",
    "h_time_series_1_ds = Dataset.get_by_name(ws,\"h_time_series_1\")\n",
    "h_time_series_2_ds = Dataset.get_by_name(ws,\"h_time_series_2\")\n",
    "h_time_series_3_ds = Dataset.get_by_name(ws,\"h_time_series_3\")\n",
    "# daily time series\n",
    "d_time_series_1_dr = DataReference(datastore=blob_store,\n",
    "                                   data_reference_name=\"d_time_series_1\",\n",
    "                                   path_on_datastore=\"datasets/time-series/X1.csv\")\n",
    "d_time_series_2_dr = DataReference(datastore=blob_store,\n",
    "                                   data_reference_name=\"d_time_series_2\",\n",
    "                                   path_on_datastore=\"datasets/time-series/X2.csv\")\n",
    "\n",
    "print(\"== Datasets metadata retrieved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermediate/Output Data\n",
    "Intermediate data (or output of a Step) is represented by **[PipelineData](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinedata?view=azure-ml-py)** object. PipelineData can be produced by one step and consumed in another step by providing the PipelineData object as an output of one step and the input of one or more steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Intermediate PipelineData references created\n"
     ]
    }
   ],
   "source": [
    "# Define intermediate data using PipelineData\n",
    "# hourly series that need to have timestampts generated from columns and pivoted if necessary\n",
    "h_time_series_1_pivot_pd = PipelineData(\"h_time_series_1_pivot\",datastore=blob_store)\n",
    "h_time_series_2_pivot_pd = PipelineData(\"h_time_series_2_pivot\",datastore=blob_store)\n",
    "h_time_series_3_pivot_pd = PipelineData(\"h_time_series_3_pivot\",datastore=blob_store)\n",
    "print(\"== Intermediate PipelineData references created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines steps using datasources and intermediate data\n",
    "Machine learning pipelines can have many steps and these steps could use or reuse datasources and intermediate data. Here's how we construct such a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best practice is to use separate folders for scripts and its dependent files\n",
    "# for each step and specify that folder as the source_directory for the step.\n",
    "# This helps reduce the size of the snapshot created for the step (only the specific folder is snapshotted).\n",
    "# Since changes in any files in the source_directory would trigger a re-upload of the snapshot, this helps\n",
    "# keep the reuse of the step when there are no changes in the source_directory of the step.\n",
    "source_directory_pivot = 'src/pivot'\n",
    "source_directory_join = 'src/join'\n",
    "source_directory_groupby_aggregate = 'src/groupby-aggregate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== PythonScriptStep time_series_1_pivot_step created\n",
      "== PythonScriptStep time_series_2_pivot_step created\n",
      "== PythonScriptStep time_series_3_pivot_step created\n"
     ]
    }
   ],
   "source": [
    "# step4 consumes the datasource (Datareference) in the previous step\n",
    "# and produces processed_data1\n",
    "h_time_series_1_pivot_step = PythonScriptStep(\n",
    "    script_name=\"pivot.py\", \n",
    "    arguments=[\"--date_column\",\"MYDATE\",\n",
    "               \"--hour_column\",\"HOUR\",\n",
    "               \"--datetime_column_name\",\"DATETIME\",\n",
    "               \"--pivot_columns\",\"NODE_ID\",\n",
    "               \"--value_column\",\"MW\",\n",
    "               \"--output\", h_time_series_1_pivot_pd],\n",
    "    inputs=[h_time_series_1_ds.as_named_input(\"time_series\")],\n",
    "    outputs=[h_time_series_1_pivot_pd],\n",
    "    compute_target=aml_compute, \n",
    "    source_directory=source_directory_pivot,\n",
    "    runconfig=run_config\n",
    ")\n",
    "print(\"== PythonScriptStep time_series_1_pivot_step created\")\n",
    "\n",
    "h_time_series_2_pivot_step = PythonScriptStep(\n",
    "    script_name=\"pivot.py\", \n",
    "    arguments=[\"--date_column\",\"MYDATE\",\n",
    "               \"--hour_column\",\"HOUR\",\n",
    "               \"--datetime_column_name\",\"DATETIME\",\n",
    "               \"--pivot_columns\",\"NODE_ID\",\n",
    "               \"--value_column\",\"MW\",\n",
    "               \"--output\", h_time_series_2_pivot_pd],\n",
    "    inputs=[h_time_series_2_ds.as_named_input(\"time_series\")],\n",
    "    outputs=[h_time_series_2_pivot_pd],\n",
    "    compute_target=aml_compute, \n",
    "    source_directory=source_directory_pivot,\n",
    "    runconfig=run_config\n",
    ")\n",
    "print(\"== PythonScriptStep time_series_2_pivot_step created\")\n",
    "\n",
    "h_time_series_3_pivot_step = PythonScriptStep(\n",
    "    script_name=\"pivot.py\", \n",
    "    arguments=[\"--date_column\",\"DATE\",\n",
    "               \"--hour_column\",\"HE\",\n",
    "               \"--datetime_column_name\",\"DATETIME\",\n",
    "               \"--pivot_columns\",\"\",\n",
    "               \"--value_column\",\"CLOAD\",\n",
    "               \"--output\", h_time_series_3_pivot_pd],\n",
    "    inputs=[h_time_series_3_ds.as_named_input(\"time_series\")],\n",
    "    outputs=[h_time_series_3_pivot_pd],\n",
    "    compute_target=aml_compute, \n",
    "    source_directory=source_directory_pivot,\n",
    "    runconfig=run_config\n",
    ")\n",
    "print(\"== PythonScriptStep time_series_3_pivot_step created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a Step that consumes intermediate data and produces intermediate data\n",
    "In this step, we define a step that consumes an intermediate data and produces intermediate data.\n",
    "\n",
    "**Open `join.py` in the local machine and examine the arguments, inputs, and outputs for the script. That will give you a good sense of why the script argument names used below are important.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== PythonScriptStep h_join_step created\n"
     ]
    }
   ],
   "source": [
    "# Joining all HOURLY pivoted time-series\n",
    "h_time_series_joined_ds = PipelineData('h_time_series_joined', datastore=blob_store).as_dataset()\n",
    "h_time_series_joined_ds = h_time_series_joined_ds.register(name='h_time_series_joined', create_new_version=True)\n",
    "\n",
    "# Join Step\n",
    "h_join_step = PythonScriptStep(\n",
    "    script_name=\"join.py\",\n",
    "    arguments=[\"--join_column\", \"DATETIME\",\n",
    "               \"--input_1\", h_time_series_1_pivot_pd,\n",
    "               \"--input_2\", h_time_series_2_pivot_pd,\n",
    "               \"--input_3\", h_time_series_3_pivot_pd,\n",
    "               \"--output\", h_time_series_joined_ds],\n",
    "    inputs=[h_time_series_1_pivot_pd,\n",
    "            h_time_series_2_pivot_pd,\n",
    "            h_time_series_3_pivot_pd],\n",
    "    outputs=[h_time_series_joined_ds],\n",
    "    compute_target=aml_compute, \n",
    "    source_directory=source_directory_join)\n",
    "\n",
    "print(\"== PythonScriptStep h_join_step created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== PythonScriptStep d_join_step created\n"
     ]
    }
   ],
   "source": [
    "# Joining all DAILY time-series\n",
    "d_time_series_joined_ds = PipelineData('d_time_series_joined', datastore=blob_store).as_dataset()\n",
    "d_time_series_joined_ds = d_time_series_joined_ds.register(name='d_time_series_joined', create_new_version=True)\n",
    "\n",
    "# Join Step\n",
    "d_join_step = PythonScriptStep(\n",
    "    script_name=\"join.py\",\n",
    "    arguments=[\"--join_column\", \"RDATE\",\n",
    "               \"--input_1\", d_time_series_1_dr,\n",
    "               \"--input_2\", d_time_series_2_dr,\n",
    "               \"--output\", d_time_series_joined_ds],\n",
    "    inputs=[d_time_series_1_dr,\n",
    "            d_time_series_2_dr],\n",
    "    outputs=[d_time_series_joined_ds],\n",
    "    compute_target=aml_compute,\n",
    "    source_directory=source_directory_join)\n",
    "\n",
    "print(\"== PythonScriptStep d_join_step created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== PythonScriptStep groupby_aggregate_step created\n"
     ]
    }
   ],
   "source": [
    "# Generate stats for HOURLY joined series GROUP BY DAILY\n",
    "h_time_series_joined_groupby_aggregated_ds = PipelineData('h_time_series_joined_groupby_aggregated', datastore=blob_store).as_dataset()\n",
    "h_time_series_joined_groupby_aggregated_ds = h_time_series_joined_groupby_aggregated_ds.register(name='h_time_series_joined_groupby_aggregated', create_new_version=True)\n",
    "\n",
    "# groupby-aggregate step\n",
    "groupby_aggregate_step = PythonScriptStep(\n",
    "    script_name=\"groupby-aggregate.py\",\n",
    "    arguments=[\"--datetime_column\", \"DATETIME\",\n",
    "               \"--date_column_name\",\"RDATE\",\n",
    "               \"--input_ds\", h_time_series_joined_ds,\n",
    "               \"--output_ds\", h_time_series_joined_groupby_aggregated_ds],\n",
    "    inputs=[h_time_series_joined_ds],\n",
    "    outputs=[h_time_series_joined_groupby_aggregated_ds],\n",
    "    compute_target=aml_compute,\n",
    "    source_directory=source_directory_groupby_aggregate)\n",
    "\n",
    "print(\"== PythonScriptStep groupby_aggregate_step created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the pipeline and submit an Experiment run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Pipeline is built\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[h_join_step,d_join_step,groupby_aggregate_step])\n",
    "print (\"== Pipeline is built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step join.py [aa7b6a37][237a17df-f481-4c9b-a503-6e573d32b214], (This step is eligible to reuse a previous run's output)\n",
      "Created step pivot.py [445b729a][bfd9831a-22db-49f9-b273-5d944f34e29a], (This step is eligible to reuse a previous run's output)\n",
      "Created step pivot.py [866692d3][1855b634-0083-4a72-8a18-eaeda07375d1], (This step is eligible to reuse a previous run's output)\n",
      "Created step pivot.py [74f2e610][2a23178d-0e17-4791-9475-fba2a1f33bcb], (This step is eligible to reuse a previous run's output)\n",
      "Created step join.py [c5ae7802][7842a40d-bfd6-455c-9dd7-7667a5d80aec], (This step is eligible to reuse a previous run's output)\n",
      "Created step groupby-aggregate.py [069ed222][1b3761bf-34b3-4c61-9df4-5b06df0cd96a], (This step is eligible to reuse a previous run's output)\n",
      "Using data reference d_time_series_1 for StepId [dfde5f9b][466adead-675c-4485-b1f6-3a08282a68dd], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference d_time_series_2 for StepId [c82deab7][0317946d-9399-4b25-a6c8-30e0e608776b], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Submitted PipelineRun 65c5d560-c39d-4810-959b-5d9ead59e1a4\n",
      "Link to Azure Machine Learning studio: https://ml.azure.com/experiments/use-case-1-data-prep/runs/65c5d560-c39d-4810-959b-5d9ead59e1a4?wsid=/subscriptions/c5ec24ce-9c5f-4da2-bf12-9ca8e9758d60/resourcegroups/azure-ml-workshop/workspaces/agd-ml-demo\n",
      "== Pipeline is submitted for execution\n"
     ]
    }
   ],
   "source": [
    "pipeline_run = Experiment(ws, 'use-case-1-data-prep').submit(pipeline)\n",
    "print(\"== Pipeline is submitted for execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b418a33f733c4a91a58de55da853be26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Failed\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/use-case-1-data-prep/runs/65c5d560-c39d-4810-959b-5d9ead59e1a4?wsid=/subscriptions/c5ec24ce-9c5f-4da2-bf12-9ca8e9758d60/resourcegroups/azure-ml-workshop/workspaces/agd-ml-demo\", \"run_id\": \"65c5d560-c39d-4810-959b-5d9ead59e1a4\", \"run_properties\": {\"run_id\": \"65c5d560-c39d-4810-959b-5d9ead59e1a4\", \"created_utc\": \"2020-04-02T17:29:57.696106Z\", \"properties\": {\"azureml.runsource\": \"azureml.PipelineRun\", \"runSource\": \"SDK\", \"runType\": \"SDK\", \"azureml.parameters\": \"{}\"}, \"tags\": {\"azureml.pipelineComponent\": \"pipelinerun\"}, \"end_time_utc\": \"2020-04-02T17:32:04.102175Z\", \"status\": \"Failed\", \"log_files\": {\"logs/azureml/executionlogs.txt\": \"https://agdmldemo9837909182.blob.core.windows.net/azureml/ExperimentRun/dcid.65c5d560-c39d-4810-959b-5d9ead59e1a4/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=wq%2BDa58SK3RtyLogGyvmy%2FL%2Bs%2FU2OVszACLu7gLE82k%3D&st=2020-04-02T17%3A22%3A26Z&se=2020-04-03T01%3A32%3A26Z&sp=r\", \"logs/azureml/stderrlogs.txt\": \"https://agdmldemo9837909182.blob.core.windows.net/azureml/ExperimentRun/dcid.65c5d560-c39d-4810-959b-5d9ead59e1a4/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=TMboc2zDUrFCk1ah03h%2BangObjZFs7bnThIlFtclilY%3D&st=2020-04-02T17%3A22%3A26Z&se=2020-04-03T01%3A32%3A26Z&sp=r\", \"logs/azureml/stdoutlogs.txt\": \"https://agdmldemo9837909182.blob.core.windows.net/azureml/ExperimentRun/dcid.65c5d560-c39d-4810-959b-5d9ead59e1a4/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=ysg%2BfpPX7gzffM%2Fj%2F0eq27bpgGSe56%2FYYHmJ2rOLq1E%3D&st=2020-04-02T17%3A22%3A26Z&se=2020-04-03T01%3A32%3A26Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/executionlogs.txt\", \"logs/azureml/stderrlogs.txt\", \"logs/azureml/stdoutlogs.txt\"]], \"run_duration\": \"0:02:06\"}, \"child_runs\": [{\"run_id\": \"\", \"name\": \"join.py\", \"status\": \"NotStarted\", \"start_time\": \"\", \"created_time\": \"\", \"end_time\": \"\", \"duration\": \"\"}, {\"run_id\": \"d3accc73-c4b5-4ac5-a2d0-5c86c369dc71\", \"name\": \"pivot.py\", \"status\": \"Canceled\", \"start_time\": \"2020-04-02T17:31:01.501219Z\", \"created_time\": \"2020-04-02T17:30:17.82904Z\", \"end_time\": \"2020-04-02T17:32:21.073901Z\", \"duration\": \"0:02:03\", \"run_number\": 8, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-04-02T17:30:17.82904Z\", \"is_reused\": \"\"}, {\"run_id\": \"f630b51f-b7cc-4e6b-ad93-a50404397b31\", \"name\": \"pivot.py\", \"status\": \"Failed\", \"start_time\": \"2020-04-02T17:31:00.899115Z\", \"created_time\": \"2020-04-02T17:30:18.155119Z\", \"end_time\": \"2020-04-02T17:31:58.328587Z\", \"duration\": \"0:01:40\", \"run_number\": 10, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-04-02T17:30:18.155119Z\", \"is_reused\": \"\"}, {\"run_id\": \"edfc3a99-126a-4bdb-981c-6d3de72d555f\", \"name\": \"pivot.py\", \"status\": \"Canceled\", \"start_time\": \"2020-04-02T17:31:03.650796Z\", \"created_time\": \"2020-04-02T17:30:17.840532Z\", \"end_time\": \"\", \"duration\": \"0:02:10\", \"run_number\": 9, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-04-02T17:30:17.840532Z\", \"is_reused\": \"\"}, {\"run_id\": \"f82fb7d4-42a8-4b21-8149-ef3a6fbeceb1\", \"name\": \"join.py\", \"status\": \"Finished\", \"start_time\": \"2020-04-02T17:30:17.304611Z\", \"created_time\": \"2020-04-02T17:30:17.304611Z\", \"end_time\": \"2020-04-02T17:30:17.389038Z\", \"duration\": \"0:00:00\", \"run_number\": 7, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-04-02T17:30:17.304611Z\", \"is_reused\": \"Yes\"}, {\"run_id\": \"\", \"name\": \"groupby-aggregate.py\", \"status\": \"NotStarted\", \"start_time\": \"\", \"created_time\": \"\", \"end_time\": \"\", \"duration\": \"\"}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2020-04-02 17:30:17Z] Completing processing run id f82fb7d4-42a8-4b21-8149-ef3a6fbeceb1.\\n[2020-04-02 17:30:17Z] Submitting 3 runs, first five are: 445b729a:d3accc73-c4b5-4ac5-a2d0-5c86c369dc71,74f2e610:edfc3a99-126a-4bdb-981c-6d3de72d555f,866692d3:f630b51f-b7cc-4e6b-ad93-a50404397b31\\n[2020-04-02 17:32:03Z] Execution of experiment failed, update experiment status and cancel running nodes.\\n\", \"graph\": {\"datasource_nodes\": {\"84cc5485\": {\"node_id\": \"84cc5485\", \"name\": \"h_time_series_1\"}, \"36a3aa54\": {\"node_id\": \"36a3aa54\", \"name\": \"h_time_series_2\"}, \"dad57895\": {\"node_id\": \"dad57895\", \"name\": \"h_time_series_3\"}, \"dfde5f9b\": {\"node_id\": \"dfde5f9b\", \"name\": \"d_time_series_1\"}, \"c82deab7\": {\"node_id\": \"c82deab7\", \"name\": \"d_time_series_2\"}}, \"module_nodes\": {\"aa7b6a37\": {\"node_id\": \"aa7b6a37\", \"name\": \"join.py\", \"status\": \"NotStarted\"}, \"445b729a\": {\"node_id\": \"445b729a\", \"name\": \"pivot.py\", \"status\": \"Canceled\", \"_is_reused\": false, \"run_id\": \"d3accc73-c4b5-4ac5-a2d0-5c86c369dc71\"}, \"866692d3\": {\"node_id\": \"866692d3\", \"name\": \"pivot.py\", \"status\": \"Failed\", \"_is_reused\": false, \"run_id\": \"f630b51f-b7cc-4e6b-ad93-a50404397b31\"}, \"74f2e610\": {\"node_id\": \"74f2e610\", \"name\": \"pivot.py\", \"status\": \"Canceled\", \"_is_reused\": false, \"run_id\": \"edfc3a99-126a-4bdb-981c-6d3de72d555f\"}, \"c5ae7802\": {\"node_id\": \"c5ae7802\", \"name\": \"join.py\", \"status\": \"Finished\", \"_is_reused\": true, \"run_id\": \"f82fb7d4-42a8-4b21-8149-ef3a6fbeceb1\"}, \"069ed222\": {\"node_id\": \"069ed222\", \"name\": \"groupby-aggregate.py\", \"status\": \"NotStarted\"}}, \"edges\": [{\"source_node_id\": \"445b729a\", \"source_node_name\": \"pivot.py\", \"source_name\": \"h_time_series_1_pivot\", \"target_name\": \"h_time_series_1_pivot\", \"dst_node_id\": \"aa7b6a37\", \"dst_node_name\": \"join.py\"}, {\"source_node_id\": \"866692d3\", \"source_node_name\": \"pivot.py\", \"source_name\": \"h_time_series_2_pivot\", \"target_name\": \"h_time_series_1_pivot\", \"dst_node_id\": \"aa7b6a37\", \"dst_node_name\": \"join.py\"}, {\"source_node_id\": \"74f2e610\", \"source_node_name\": \"pivot.py\", \"source_name\": \"h_time_series_3_pivot\", \"target_name\": \"h_time_series_1_pivot\", \"dst_node_id\": \"aa7b6a37\", \"dst_node_name\": \"join.py\"}, {\"source_node_id\": \"84cc5485\", \"source_node_name\": \"h_time_series_1\", \"source_name\": \"data\", \"target_name\": \"time_series\", \"dst_node_id\": \"445b729a\", \"dst_node_name\": \"pivot.py\"}, {\"source_node_id\": \"36a3aa54\", \"source_node_name\": \"h_time_series_2\", \"source_name\": \"data\", \"target_name\": \"time_series\", \"dst_node_id\": \"866692d3\", \"dst_node_name\": \"pivot.py\"}, {\"source_node_id\": \"dad57895\", \"source_node_name\": \"h_time_series_3\", \"source_name\": \"data\", \"target_name\": \"time_series\", \"dst_node_id\": \"74f2e610\", \"dst_node_name\": \"pivot.py\"}, {\"source_node_id\": \"dfde5f9b\", \"source_node_name\": \"d_time_series_1\", \"source_name\": \"data\", \"target_name\": \"d_time_series_1\", \"dst_node_id\": \"c5ae7802\", \"dst_node_name\": \"join.py\"}, {\"source_node_id\": \"c82deab7\", \"source_node_name\": \"d_time_series_2\", \"source_name\": \"data\", \"target_name\": \"d_time_series_1\", \"dst_node_id\": \"c5ae7802\", \"dst_node_name\": \"join.py\"}, {\"source_node_id\": \"aa7b6a37\", \"source_node_name\": \"join.py\", \"source_name\": \"h_time_series_joined\", \"target_name\": \"h_time_series_joined\", \"dst_node_id\": \"069ed222\", \"dst_node_name\": \"groupby-aggregate.py\"}], \"child_runs\": [{\"run_id\": \"\", \"name\": \"join.py\", \"status\": \"NotStarted\", \"start_time\": \"\", \"created_time\": \"\", \"end_time\": \"\", \"duration\": \"\"}, {\"run_id\": \"d3accc73-c4b5-4ac5-a2d0-5c86c369dc71\", \"name\": \"pivot.py\", \"status\": \"Canceled\", \"start_time\": \"2020-04-02T17:31:01.501219Z\", \"created_time\": \"2020-04-02T17:30:17.82904Z\", \"end_time\": \"2020-04-02T17:32:21.073901Z\", \"duration\": \"0:02:03\", \"run_number\": 8, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-04-02T17:30:17.82904Z\", \"is_reused\": \"\"}, {\"run_id\": \"f630b51f-b7cc-4e6b-ad93-a50404397b31\", \"name\": \"pivot.py\", \"status\": \"Failed\", \"start_time\": \"2020-04-02T17:31:00.899115Z\", \"created_time\": \"2020-04-02T17:30:18.155119Z\", \"end_time\": \"2020-04-02T17:31:58.328587Z\", \"duration\": \"0:01:40\", \"run_number\": 10, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-04-02T17:30:18.155119Z\", \"is_reused\": \"\"}, {\"run_id\": \"edfc3a99-126a-4bdb-981c-6d3de72d555f\", \"name\": \"pivot.py\", \"status\": \"Canceled\", \"start_time\": \"2020-04-02T17:31:03.650796Z\", \"created_time\": \"2020-04-02T17:30:17.840532Z\", \"end_time\": \"\", \"duration\": \"0:02:10\", \"run_number\": 9, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-04-02T17:30:17.840532Z\", \"is_reused\": \"\"}, {\"run_id\": \"f82fb7d4-42a8-4b21-8149-ef3a6fbeceb1\", \"name\": \"join.py\", \"status\": \"Finished\", \"start_time\": \"2020-04-02T17:30:17.304611Z\", \"created_time\": \"2020-04-02T17:30:17.304611Z\", \"end_time\": \"2020-04-02T17:30:17.389038Z\", \"duration\": \"0:00:00\", \"run_number\": 7, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-04-02T17:30:17.304611Z\", \"is_reused\": \"Yes\"}, {\"run_id\": \"\", \"name\": \"groupby-aggregate.py\", \"status\": \"NotStarted\", \"start_time\": \"\", \"created_time\": \"\", \"end_time\": \"\", \"duration\": \"\"}]}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.0.85\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for pipeline run to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineRunId: 65c5d560-c39d-4810-959b-5d9ead59e1a4\n",
      "Link to Portal: https://ml.azure.com/experiments/use-case-1-data-prep/runs/65c5d560-c39d-4810-959b-5d9ead59e1a4?wsid=/subscriptions/c5ec24ce-9c5f-4da2-bf12-9ca8e9758d60/resourcegroups/azure-ml-workshop/workspaces/agd-ml-demo\n",
      "PipelineRun Status: NotStarted\n",
      "PipelineRun Status: Running\n",
      "\n",
      "\n",
      "StepRunId: f82fb7d4-42a8-4b21-8149-ef3a6fbeceb1\n",
      "Link to Portal: https://ml.azure.com/experiments/use-case-1-data-prep/runs/f82fb7d4-42a8-4b21-8149-ef3a6fbeceb1?wsid=/subscriptions/c5ec24ce-9c5f-4da2-bf12-9ca8e9758d60/resourcegroups/azure-ml-workshop/workspaces/agd-ml-demo\n",
      "\n",
      "StepRun(join.py) Execution Summary\n",
      "===================================\n",
      "StepRun( join.py ) Status: Finished\n",
      "{'runId': 'f82fb7d4-42a8-4b21-8149-ef3a6fbeceb1', 'target': 'agd-training-cpu', 'status': 'Completed', 'startTimeUtc': '2020-04-02T17:30:17.304611Z', 'endTimeUtc': '2020-04-02T17:30:17.389038Z', 'properties': {'azureml.reusedrunid': '52dc4e46-6df5-40fe-aef3-ec6c04848ca4', 'azureml.reusednodeid': '334ea4e5', 'azureml.reusedpipeline': '56b65463-e53d-4812-99e8-8578ef7cc456', 'azureml.reusedpipelinerunid': '56b65463-e53d-4812-99e8-8578ef7cc456', 'azureml.runsource': 'azureml.StepRun', 'azureml.nodeid': 'c5ae7802', 'ContentSnapshotId': '99d2af1a-8169-48c8-aabb-08a17d8213e4', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.pipelinerunid': '65c5d560-c39d-4810-959b-5d9ead59e1a4', '_azureml.ComputeTargetType': 'amlcompute', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}, 'inputDatasets': [], 'runDefinition': {'script': 'join.py', 'useAbsolutePath': False, 'arguments': ['--join_column', 'RDATE', '--input_1', '$AZUREML_DATAREFERENCE_d_time_series_1', '--input_2', '$AZUREML_DATAREFERENCE_d_time_series_2', '--output', '$AZUREML_DATAREFERENCE_d_time_series_joined'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'agd-training-cpu', 'dataReferences': {'d_time_series_1': {'dataStoreName': 'workspaceblobstore', 'mode': 'Mount', 'pathOnDataStore': 'datasets/time-series/X1.csv', 'pathOnCompute': None, 'overwrite': False}, 'd_time_series_2': {'dataStoreName': 'workspaceblobstore', 'mode': 'Mount', 'pathOnDataStore': 'datasets/time-series/X2.csv', 'pathOnCompute': None, 'overwrite': False}, 'd_time_series_joined': {'dataStoreName': 'workspaceblobstore', 'mode': 'Mount', 'pathOnDataStore': 'azureml/52dc4e46-6df5-40fe-aef3-ec6c04848ca4/d_time_series_joined', 'pathOnCompute': None, 'overwrite': False}}, 'data': {}, 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'environment': {'name': 'Experiment use-case-1-data-prep Environment', 'version': 'Autosave_2020-04-02T17:22:21Z_85677414', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'channels': ['conda-forge'], 'dependencies': ['python=3.6.2', {'pip': ['azureml-defaults']}], 'name': 'azureml_1b417bb747e35859ebf611fb43071e9c'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04', 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': True, 'shmSize': '1g'}, 'spark': {'repositories': ['[]'], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs']}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': True, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}}, 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_9cd6d8fefdd3b85cdcfe9781a29be00bfdeebd60a51533b92bfe11c3785895b3_d.txt': 'https://agdmldemo9837909182.blob.core.windows.net/azureml/ExperimentRun/dcid.52dc4e46-6df5-40fe-aef3-ec6c04848ca4/azureml-logs/55_azureml-execution-tvmps_9cd6d8fefdd3b85cdcfe9781a29be00bfdeebd60a51533b92bfe11c3785895b3_d.txt?sv=2019-02-02&sr=b&sig=vt1ShjCCindwl%2Bd2ueoKWVLr8Ib847K17OcICOTLpzE%3D&st=2020-04-02T17%3A20%3A18Z&se=2020-04-03T01%3A30%3A18Z&sp=r', 'azureml-logs/65_job_prep-tvmps_9cd6d8fefdd3b85cdcfe9781a29be00bfdeebd60a51533b92bfe11c3785895b3_d.txt': 'https://agdmldemo9837909182.blob.core.windows.net/azureml/ExperimentRun/dcid.52dc4e46-6df5-40fe-aef3-ec6c04848ca4/azureml-logs/65_job_prep-tvmps_9cd6d8fefdd3b85cdcfe9781a29be00bfdeebd60a51533b92bfe11c3785895b3_d.txt?sv=2019-02-02&sr=b&sig=FKp%2FyySmsu1g9WNKUd3Aq%2FAkMzVThjSmI5qzuCbR1%2F8%3D&st=2020-04-02T17%3A20%3A18Z&se=2020-04-03T01%3A30%3A18Z&sp=r', 'azureml-logs/70_driver_log.txt': 'https://agdmldemo9837909182.blob.core.windows.net/azureml/ExperimentRun/dcid.52dc4e46-6df5-40fe-aef3-ec6c04848ca4/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=o%2BLWQ2SHSDJQljYFqfh%2FG%2B6612DSPfIBMfb19CSzuWY%3D&st=2020-04-02T17%3A20%3A18Z&se=2020-04-03T01%3A30%3A18Z&sp=r', 'azureml-logs/75_job_post-tvmps_9cd6d8fefdd3b85cdcfe9781a29be00bfdeebd60a51533b92bfe11c3785895b3_d.txt': 'https://agdmldemo9837909182.blob.core.windows.net/azureml/ExperimentRun/dcid.52dc4e46-6df5-40fe-aef3-ec6c04848ca4/azureml-logs/75_job_post-tvmps_9cd6d8fefdd3b85cdcfe9781a29be00bfdeebd60a51533b92bfe11c3785895b3_d.txt?sv=2019-02-02&sr=b&sig=FAg9nzKakJnNFdx3ZbaBtFAYHwweQ9%2BuwRxgzH3XIDI%3D&st=2020-04-02T17%3A20%3A18Z&se=2020-04-03T01%3A30%3A18Z&sp=r', 'azureml-logs/process_info.json': 'https://agdmldemo9837909182.blob.core.windows.net/azureml/ExperimentRun/dcid.52dc4e46-6df5-40fe-aef3-ec6c04848ca4/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=cPUDoTVHlawiZwHPNcZx11Jj8XpH4xws1zyNHHK7jyk%3D&st=2020-04-02T17%3A20%3A18Z&se=2020-04-03T01%3A30%3A18Z&sp=r', 'azureml-logs/process_status.json': 'https://agdmldemo9837909182.blob.core.windows.net/azureml/ExperimentRun/dcid.52dc4e46-6df5-40fe-aef3-ec6c04848ca4/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=OMFnRaSS889UQwsm4upFS3Very0oC91%2Falryu6%2Bs698%3D&st=2020-04-02T17%3A20%3A18Z&se=2020-04-03T01%3A30%3A18Z&sp=r', 'logs/azureml/139_azureml.log': 'https://agdmldemo9837909182.blob.core.windows.net/azureml/ExperimentRun/dcid.52dc4e46-6df5-40fe-aef3-ec6c04848ca4/logs/azureml/139_azureml.log?sv=2019-02-02&sr=b&sig=uVF6NvMe%2BeIKf%2FptAcj%2F0TA9Q6paFXfkrbMLPHbt1Kg%3D&st=2020-04-02T17%3A20%3A17Z&se=2020-04-03T01%3A30%3A17Z&sp=r', 'logs/azureml/azureml.log': 'https://agdmldemo9837909182.blob.core.windows.net/azureml/ExperimentRun/dcid.52dc4e46-6df5-40fe-aef3-ec6c04848ca4/logs/azureml/azureml.log?sv=2019-02-02&sr=b&sig=h8iv%2BicJzuk9eJ0p010J3d0%2FFTsz%2FkRL5TUINNLuEN4%3D&st=2020-04-02T17%3A20%3A17Z&se=2020-04-03T01%3A30%3A17Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://agdmldemo9837909182.blob.core.windows.net/azureml/ExperimentRun/dcid.52dc4e46-6df5-40fe-aef3-ec6c04848ca4/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=tQnRAzyFxsDzh0c6CydRFBrvZ%2B6p5rtKyi0GLGESLKk%3D&st=2020-04-02T17%3A20%3A18Z&se=2020-04-03T01%3A30%3A18Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://agdmldemo9837909182.blob.core.windows.net/azureml/ExperimentRun/dcid.52dc4e46-6df5-40fe-aef3-ec6c04848ca4/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=Yvl8OTH0lenL3yevGv%2BTUtbc%2FqF8Nzd1yJpQvZ4MT44%3D&st=2020-04-02T17%3A20%3A18Z&se=2020-04-03T01%3A30%3A18Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://agdmldemo9837909182.blob.core.windows.net/azureml/ExperimentRun/dcid.52dc4e46-6df5-40fe-aef3-ec6c04848ca4/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=HZOQ2yuBjAXPiOWP98GwE1DfQLz14dTycWuVl3GfNas%3D&st=2020-04-02T17%3A20%3A18Z&se=2020-04-03T01%3A30%3A18Z&sp=r'}}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "StepRunId: f630b51f-b7cc-4e6b-ad93-a50404397b31\n",
      "Link to Portal: https://ml.azure.com/experiments/use-case-1-data-prep/runs/f630b51f-b7cc-4e6b-ad93-a50404397b31?wsid=/subscriptions/c5ec24ce-9c5f-4da2-bf12-9ca8e9758d60/resourcegroups/azure-ml-workshop/workspaces/agd-ml-demo\n",
      "StepRun( pivot.py ) Status: NotStarted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StepRun( pivot.py ) Status: Running\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_8d61c9874ff3d5b1f6b6b1c4382e01d968590c71c842d13c88a40d456aa5093e_d.txt\n",
      "========================================================================================================================\n",
      "2020-04-02T17:31:03Z Starting output-watcher...\n",
      "2020-04-02T17:31:03Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_fbecfa6255cba4c1cc15571eceadee24\n",
      "Digest: sha256:594ef1dded733ed51cf75ec7f5bd4227657d6aec1dba0d03763c299995ee1b79\n",
      "Status: Image is up to date for agdmldemoaf0903d3.azurecr.io/azureml/azureml_fbecfa6255cba4c1cc15571eceadee24:latest\n",
      "7108642d118492598d1dae4ec48dd165c774308ff811b96412d1e406292aa99a\n",
      "2020/04/02 17:31:05 Version: 3.0.01172.0001 Branch: master Commit: d33e301a\n",
      "2020/04/02 17:31:06 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      "2020/04/02 17:31:06 sshd runtime has already been installed in the container\n",
      "\n",
      "Streaming azureml-logs/65_job_prep-tvmps_8d61c9874ff3d5b1f6b6b1c4382e01d968590c71c842d13c88a40d456aa5093e_d.txt\n",
      "===============================================================================================================\n",
      "Starting job preparation. Current time:2020-04-02T17:31:15.125036\n",
      "Extracting the control code.\n",
      "fetching and extracting the control code on master node.\n",
      "Retrieving project from snapshot: 72d447cb-9e70-4774-8bcb-e16ecba95be2\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 87\n",
      "Starting project file download.\n",
      "Finished project file download.\n",
      "Download from datastores if requested.\n",
      "Acquired lockfile /tmp/f630b51f-b7cc-4e6b-ad93-a50404397b31-datastore.lock to downloading input data references\n",
      "Download or mount from datasets if requested.\n",
      "Job preparation is complete. Current time:2020-04-02T17:31:19.309469\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 144\n",
      "Entering Run History Context Manager.\n",
      "Preparing to call script [ pivot.py ] with arguments: ['--date_column', 'MYDATE', '--hour_column', 'HOUR', '--datetime_column_name', 'DATETIME', '--pivot_columns', 'NODE_ID', '--value_column', 'MW', '--output', '/mnt/batch/tasks/shared/LS_root/jobs/agd-ml-demo/azureml/f630b51f-b7cc-4e6b-ad93-a50404397b31/mounts/workspaceblobstore/azureml/f630b51f-b7cc-4e6b-ad93-a50404397b31/h_time_series_2_pivot']\n",
      "After variable expansion, calling script [ pivot.py ] with arguments: ['--date_column', 'MYDATE', '--hour_column', 'HOUR', '--datetime_column_name', 'DATETIME', '--pivot_columns', 'NODE_ID', '--value_column', 'MW', '--output', '/mnt/batch/tasks/shared/LS_root/jobs/agd-ml-demo/azureml/f630b51f-b7cc-4e6b-ad93-a50404397b31/mounts/workspaceblobstore/azureml/f630b51f-b7cc-4e6b-ad93-a50404397b31/h_time_series_2_pivot']\n",
      "\n",
      "Date Column: MYDATE\n",
      "Hour Column: HOUR\n",
      "Datetime Column Name: DATETIME\n",
      "Pivot Columns: NODE_ID\n",
      "Value Column: MW\n",
      "Output: /mnt/batch/tasks/shared/LS_root/jobs/agd-ml-demo/azureml/f630b51f-b7cc-4e6b-ad93-a50404397b31/mounts/workspaceblobstore/azureml/f630b51f-b7cc-4e6b-ad93-a50404397b31/h_time_series_2_pivot\n",
      "\n",
      "\n",
      "The experiment failed. Finalizing run...\n",
      "Cleaning up all outstanding Run operations, waiting 300.0 seconds\n",
      "2 items cleaning up...\n",
      "Cleanup took 0.2490699291229248 seconds\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 144\n",
      "Traceback (most recent call last):\n",
      "  File \"pivot.py\", line 44, in <module>\n",
      "    input_df[args.datetime_column_name] = input_df.apply(lambda x: gen_date(x[args.date_column], x[args.hour_column]), axis=1)\n",
      "  File \"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/site-packages/pandas/core/frame.py\", line 6878, in apply\n",
      "    return op.get_result()\n",
      "  File \"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/site-packages/pandas/core/apply.py\", line 186, in get_result\n",
      "    return self.apply_standard()\n",
      "  File \"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/site-packages/pandas/core/apply.py\", line 296, in apply_standard\n",
      "    values, self.f, axis=self.axis, dummy=dummy, labels=labels\n",
      "  File \"pandas/_libs/reduction.pyx\", line 620, in pandas._libs.reduction.compute_reduction\n",
      "  File \"pandas/_libs/reduction.pyx\", line 128, in pandas._libs.reduction.Reducer.get_result\n",
      "  File \"pivot.py\", line 44, in <lambda>\n",
      "    input_df[args.datetime_column_name] = input_df.apply(lambda x: gen_date(x[args.date_column], x[args.hour_column]), axis=1)\n",
      "  File \"pivot.py\", line 17, in gen_date\n",
      "    return d + dt.timedelta(hours=int(h))\n",
      "TypeError: can only concatenate str (not \"datetime.timedelta\") to str\n",
      "\n",
      "2020/04/02 17:31:42 mpirun version string: {\n",
      "Intel(R) MPI Library for Linux* OS, Version 2018 Update 3 Build 20180411 (id: 18329)\n",
      "Copyright 2003-2018 Intel Corporation.\n",
      "}\n",
      "2020/04/02 17:31:42 MPI publisher: intel ; version: 2018\n",
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_8d61c9874ff3d5b1f6b6b1c4382e01d968590c71c842d13c88a40d456aa5093e_d.txt\n",
      "===============================================================================================================\n",
      "Starting job release. Current time:2020-04-02T17:31:47.050260\n",
      "Logging experiment finalizing status in history service.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 355\n",
      "Job release is complete. Current time:2020-04-02T17:31:48.806184\n",
      "\n",
      "StepRun(pivot.py) Execution Summary\n",
      "====================================\n",
      "StepRun( pivot.py ) Status: Failed\n",
      "\n",
      "Warnings:\n",
      "{\n",
      "  \"error\": {\n",
      "    \"code\": \"UserError\",\n",
      "    \"message\": \"AzureMLCompute job failed.\\nJobFailed: Submitted script failed with a non-zero exit code; see the driver log file for details.\",\n",
      "    \"messageFormat\": null,\n",
      "    \"messageParameters\": {},\n",
      "    \"referenceCode\": null,\n",
      "    \"detailsUri\": null,\n",
      "    \"target\": null,\n",
      "    \"details\": [],\n",
      "    \"innerError\": null,\n",
      "    \"debugInfo\": null\n",
      "  },\n",
      "  \"correlation\": {\n",
      "    \"operation\": null,\n",
      "    \"request\": \"918657a36199d025\"\n",
      "  },\n",
      "  \"environment\": \"westus2\",\n",
      "  \"location\": \"westus2\",\n",
      "  \"time\": \"2020-04-02T17:31:58.2259597+00:00\"\n",
      "}\n"
     ]
    },
    {
     "ename": "ActivityFailedException",
     "evalue": "ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"User program failed with TypeError: can only concatenate str (not \\\"datetime.timedelta\\\") to str\",\n        \"detailsUri\": \"https://aka.ms/azureml-known-errors\",\n        \"details\": [],\n        \"debugInfo\": {\n            \"type\": \"TypeError\",\n            \"message\": \"can only concatenate str (not \\\"datetime.timedelta\\\") to str\",\n            \"stackTrace\": \"  File \\\"/mnt/batch/tasks/shared/LS_root/jobs/agd-ml-demo/azureml/f630b51f-b7cc-4e6b-ad93-a50404397b31/mounts/workspaceblobstore/azureml/f630b51f-b7cc-4e6b-ad93-a50404397b31/azureml-setup/context_manager_injector.py\\\", line 127, in execute_with_context\\n    runpy.run_path(sys.argv[0], globals(), run_name=\\\"__main__\\\")\\n  File \\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/runpy.py\\\", line 263, in run_path\\n    pkg_name=pkg_name, script_name=fname)\\n  File \\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/runpy.py\\\", line 96, in _run_module_code\\n    mod_name, mod_spec, pkg_name, script_name)\\n  File \\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/runpy.py\\\", line 85, in _run_code\\n    exec(code, run_globals)\\n  File \\\"pivot.py\\\", line 44, in <module>\\n    input_df[args.datetime_column_name] = input_df.apply(lambda x: gen_date(x[args.date_column], x[args.hour_column]), axis=1)\\n  File \\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/site-packages/pandas/core/frame.py\\\", line 6878, in apply\\n    return op.get_result()\\n  File \\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/site-packages/pandas/core/apply.py\\\", line 186, in get_result\\n    return self.apply_standard()\\n  File \\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/site-packages/pandas/core/apply.py\\\", line 296, in apply_standard\\n    values, self.f, axis=self.axis, dummy=dummy, labels=labels\\n  File \\\"pandas/_libs/reduction.pyx\\\", line 620, in pandas._libs.reduction.compute_reduction\\n  File \\\"pandas/_libs/reduction.pyx\\\", line 128, in pandas._libs.reduction.Reducer.get_result\\n  File \\\"pivot.py\\\", line 44, in <lambda>\\n    input_df[args.datetime_column_name] = input_df.apply(lambda x: gen_date(x[args.date_column], x[args.hour_column]), axis=1)\\n  File \\\"pivot.py\\\", line 17, in gen_date\\n    return d + dt.timedelta(hours=int(h))\\n\"\n        }\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"User program failed with TypeError: can only concatenate str (not \\\\\\\"datetime.timedelta\\\\\\\") to str\\\",\\n        \\\"detailsUri\\\": \\\"https://aka.ms/azureml-known-errors\\\",\\n        \\\"details\\\": [],\\n        \\\"debugInfo\\\": {\\n            \\\"type\\\": \\\"TypeError\\\",\\n            \\\"message\\\": \\\"can only concatenate str (not \\\\\\\"datetime.timedelta\\\\\\\") to str\\\",\\n            \\\"stackTrace\\\": \\\"  File \\\\\\\"/mnt/batch/tasks/shared/LS_root/jobs/agd-ml-demo/azureml/f630b51f-b7cc-4e6b-ad93-a50404397b31/mounts/workspaceblobstore/azureml/f630b51f-b7cc-4e6b-ad93-a50404397b31/azureml-setup/context_manager_injector.py\\\\\\\", line 127, in execute_with_context\\\\n    runpy.run_path(sys.argv[0], globals(), run_name=\\\\\\\"__main__\\\\\\\")\\\\n  File \\\\\\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/runpy.py\\\\\\\", line 263, in run_path\\\\n    pkg_name=pkg_name, script_name=fname)\\\\n  File \\\\\\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/runpy.py\\\\\\\", line 96, in _run_module_code\\\\n    mod_name, mod_spec, pkg_name, script_name)\\\\n  File \\\\\\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/runpy.py\\\\\\\", line 85, in _run_code\\\\n    exec(code, run_globals)\\\\n  File \\\\\\\"pivot.py\\\\\\\", line 44, in <module>\\\\n    input_df[args.datetime_column_name] = input_df.apply(lambda x: gen_date(x[args.date_column], x[args.hour_column]), axis=1)\\\\n  File \\\\\\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/site-packages/pandas/core/frame.py\\\\\\\", line 6878, in apply\\\\n    return op.get_result()\\\\n  File \\\\\\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/site-packages/pandas/core/apply.py\\\\\\\", line 186, in get_result\\\\n    return self.apply_standard()\\\\n  File \\\\\\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/site-packages/pandas/core/apply.py\\\\\\\", line 296, in apply_standard\\\\n    values, self.f, axis=self.axis, dummy=dummy, labels=labels\\\\n  File \\\\\\\"pandas/_libs/reduction.pyx\\\\\\\", line 620, in pandas._libs.reduction.compute_reduction\\\\n  File \\\\\\\"pandas/_libs/reduction.pyx\\\\\\\", line 128, in pandas._libs.reduction.Reducer.get_result\\\\n  File \\\\\\\"pivot.py\\\\\\\", line 44, in <lambda>\\\\n    input_df[args.datetime_column_name] = input_df.apply(lambda x: gen_date(x[args.date_column], x[args.hour_column]), axis=1)\\\\n  File \\\\\\\"pivot.py\\\\\\\", line 17, in gen_date\\\\n    return d + dt.timedelta(hours=int(h))\\\\n\\\"\\n        }\\n    },\\n    \\\"time\\\": \\\"0001-01-01T00:00:00.000Z\\\"\\n}\"\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mActivityFailedException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-04685269429d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/pipeline/core/run.py\u001b[0m in \u001b[0;36mwait_for_completion\u001b[0;34m(self, show_output, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    289\u001b[0m                             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                             step_run.wait_for_completion(timeout_seconds=timeout_seconds - time_elapsed,\n\u001b[0;32m--> 291\u001b[0;31m                                                          raise_on_error=raise_on_error)\n\u001b[0m\u001b[1;32m    292\u001b[0m                             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtimeout_seconds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/pipeline/core/run.py\u001b[0m in \u001b[0;36mwait_for_completion\u001b[0;34m(self, show_output, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m                 return self._stream_run_output(timeout_seconds=timeout_seconds,\n\u001b[0;32m--> 716\u001b[0;31m                                                raise_on_error=raise_on_error)\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                 \u001b[0merror_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The output streaming for the run interrupted.\\n\"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/pipeline/core/run.py\u001b[0m in \u001b[0;36m_stream_run_output\u001b[0;34m(self, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mraise_on_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mActivityFailedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_details\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_details\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mActivityFailedException\u001b[0m: ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"User program failed with TypeError: can only concatenate str (not \\\"datetime.timedelta\\\") to str\",\n        \"detailsUri\": \"https://aka.ms/azureml-known-errors\",\n        \"details\": [],\n        \"debugInfo\": {\n            \"type\": \"TypeError\",\n            \"message\": \"can only concatenate str (not \\\"datetime.timedelta\\\") to str\",\n            \"stackTrace\": \"  File \\\"/mnt/batch/tasks/shared/LS_root/jobs/agd-ml-demo/azureml/f630b51f-b7cc-4e6b-ad93-a50404397b31/mounts/workspaceblobstore/azureml/f630b51f-b7cc-4e6b-ad93-a50404397b31/azureml-setup/context_manager_injector.py\\\", line 127, in execute_with_context\\n    runpy.run_path(sys.argv[0], globals(), run_name=\\\"__main__\\\")\\n  File \\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/runpy.py\\\", line 263, in run_path\\n    pkg_name=pkg_name, script_name=fname)\\n  File \\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/runpy.py\\\", line 96, in _run_module_code\\n    mod_name, mod_spec, pkg_name, script_name)\\n  File \\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/runpy.py\\\", line 85, in _run_code\\n    exec(code, run_globals)\\n  File \\\"pivot.py\\\", line 44, in <module>\\n    input_df[args.datetime_column_name] = input_df.apply(lambda x: gen_date(x[args.date_column], x[args.hour_column]), axis=1)\\n  File \\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/site-packages/pandas/core/frame.py\\\", line 6878, in apply\\n    return op.get_result()\\n  File \\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/site-packages/pandas/core/apply.py\\\", line 186, in get_result\\n    return self.apply_standard()\\n  File \\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/site-packages/pandas/core/apply.py\\\", line 296, in apply_standard\\n    values, self.f, axis=self.axis, dummy=dummy, labels=labels\\n  File \\\"pandas/_libs/reduction.pyx\\\", line 620, in pandas._libs.reduction.compute_reduction\\n  File \\\"pandas/_libs/reduction.pyx\\\", line 128, in pandas._libs.reduction.Reducer.get_result\\n  File \\\"pivot.py\\\", line 44, in <lambda>\\n    input_df[args.datetime_column_name] = input_df.apply(lambda x: gen_date(x[args.date_column], x[args.hour_column]), axis=1)\\n  File \\\"pivot.py\\\", line 17, in gen_date\\n    return d + dt.timedelta(hours=int(h))\\n\"\n        }\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"User program failed with TypeError: can only concatenate str (not \\\\\\\"datetime.timedelta\\\\\\\") to str\\\",\\n        \\\"detailsUri\\\": \\\"https://aka.ms/azureml-known-errors\\\",\\n        \\\"details\\\": [],\\n        \\\"debugInfo\\\": {\\n            \\\"type\\\": \\\"TypeError\\\",\\n            \\\"message\\\": \\\"can only concatenate str (not \\\\\\\"datetime.timedelta\\\\\\\") to str\\\",\\n            \\\"stackTrace\\\": \\\"  File \\\\\\\"/mnt/batch/tasks/shared/LS_root/jobs/agd-ml-demo/azureml/f630b51f-b7cc-4e6b-ad93-a50404397b31/mounts/workspaceblobstore/azureml/f630b51f-b7cc-4e6b-ad93-a50404397b31/azureml-setup/context_manager_injector.py\\\\\\\", line 127, in execute_with_context\\\\n    runpy.run_path(sys.argv[0], globals(), run_name=\\\\\\\"__main__\\\\\\\")\\\\n  File \\\\\\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/runpy.py\\\\\\\", line 263, in run_path\\\\n    pkg_name=pkg_name, script_name=fname)\\\\n  File \\\\\\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/runpy.py\\\\\\\", line 96, in _run_module_code\\\\n    mod_name, mod_spec, pkg_name, script_name)\\\\n  File \\\\\\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/runpy.py\\\\\\\", line 85, in _run_code\\\\n    exec(code, run_globals)\\\\n  File \\\\\\\"pivot.py\\\\\\\", line 44, in <module>\\\\n    input_df[args.datetime_column_name] = input_df.apply(lambda x: gen_date(x[args.date_column], x[args.hour_column]), axis=1)\\\\n  File \\\\\\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/site-packages/pandas/core/frame.py\\\\\\\", line 6878, in apply\\\\n    return op.get_result()\\\\n  File \\\\\\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/site-packages/pandas/core/apply.py\\\\\\\", line 186, in get_result\\\\n    return self.apply_standard()\\\\n  File \\\\\\\"/azureml-envs/azureml_ed39ba8e8af4352edb337d7a448246d0/lib/python3.7/site-packages/pandas/core/apply.py\\\\\\\", line 296, in apply_standard\\\\n    values, self.f, axis=self.axis, dummy=dummy, labels=labels\\\\n  File \\\\\\\"pandas/_libs/reduction.pyx\\\\\\\", line 620, in pandas._libs.reduction.compute_reduction\\\\n  File \\\\\\\"pandas/_libs/reduction.pyx\\\\\\\", line 128, in pandas._libs.reduction.Reducer.get_result\\\\n  File \\\\\\\"pivot.py\\\\\\\", line 44, in <lambda>\\\\n    input_df[args.datetime_column_name] = input_df.apply(lambda x: gen_date(x[args.date_column], x[args.hour_column]), axis=1)\\\\n  File \\\\\\\"pivot.py\\\\\\\", line 17, in gen_date\\\\n    return d + dt.timedelta(hours=int(h))\\\\n\\\"\\n        }\\n    },\\n    \\\"time\\\": \\\"0001-01-01T00:00:00.000Z\\\"\\n}\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See Outputs\n",
    "\n",
    "See where outputs of each pipeline step are located on your datastore.\n",
    "\n",
    "***Wait for pipeline run to complete, to make sure all the outputs are ready***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Steps\n",
    "for step in pipeline_run.get_steps():\n",
    "    print(\"== Outputs of step \" + step.name)\n",
    "    \n",
    "    # Get a dictionary of StepRunOutputs with the output name as the key \n",
    "    output_dict = step.get_outputs()\n",
    "    \n",
    "    for name, output in output_dict.items():\n",
    "        output_reference = output.get_port_data_reference() # Get output port data reference\n",
    "        print(\"\\tname: \" + name)\n",
    "        print(\"\\tdatastore: \" + output_reference.datastore_name)\n",
    "        print(\"\\tpath on datastore: \" + output_reference.path_on_datastore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next: Publishing the Pipeline and calling it from the REST endpoint\n",
    "See this [notebook](https://aka.ms/pl-pub-rep) to understand how the pipeline is published and you can call the REST endpoint to run the pipeline."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "sanpil"
   }
  ],
  "categories": [
   "how-to-use-azureml",
   "machine-learning-pipelines",
   "intro-to-pipelines"
  ],
  "category": "tutorial",
  "compute": [
   "AML Compute"
  ],
  "datasets": [
   "Custom"
  ],
  "deployment": [
   "None"
  ],
  "exclude_from_index": false,
  "framework": [
   "Azure ML"
  ],
  "friendly_name": "Azure Machine Learning Pipelines with Data Dependency",
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "order_index": 2,
  "star_tag": [
   "featured"
  ],
  "tags": [
   "None"
  ],
  "task": "Demonstrates how to construct a Pipeline with data dependency between steps"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
