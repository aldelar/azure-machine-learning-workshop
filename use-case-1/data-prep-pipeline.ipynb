{"cells":[{"cell_type":"markdown","source":["Copyright (c) Microsoft Corporation. All rights reserved.  \n","Licensed under the MIT License."],"metadata":{}},{"cell_type":"markdown","source":["![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.png)"],"metadata":{}},{"cell_type":"markdown","source":["# Azure Machine Learning Pipelines with Data Dependency\n","In this notebook, we will see how we can build a pipeline with implicit data dependency."],"metadata":{}},{"cell_type":"markdown","source":["## Prerequisites and Azure Machine Learning Basics\n","If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the [configuration Notebook](https://aka.ms/pl-config) first if you haven't. This sets you up with a working config file that has information on your workspace, subscription id, etc. \n","\n","### Azure Machine Learning and Pipeline SDK-specific Imports"],"metadata":{}},{"cell_type":"code","source":["import azureml.core\n","from azureml.core import Workspace, Experiment, Datastore\n","from azureml.core.compute import AmlCompute\n","from azureml.core.compute import ComputeTarget\n","from azureml.widgets import RunDetails\n","\n","# Check core SDK version number\n","print(\"SDK version:\", azureml.core.VERSION)\n","\n","from azureml.data.data_reference import DataReference\n","from azureml.pipeline.core import Pipeline, PipelineData\n","from azureml.pipeline.steps import PythonScriptStep\n","print(\"Pipeline SDK-specific imports completed\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["SDK version: 1.0.83\n","Pipeline SDK-specific imports completed\n"]}],"execution_count":1,"metadata":{}},{"cell_type":"markdown","source":["### Initialize Workspace\n","\n","Initialize a [workspace](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace(class%29) object from persisted configuration."],"metadata":{}},{"cell_type":"code","source":["ws = Workspace.from_config()\n","print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n","\n","# Default datastore (Azure blob storage)\n","# def_blob_store = ws.get_default_datastore()\n","blob_store = Datastore(ws, \"workspaceblobstore\")\n","print(\"Blobstore's name: {}\".format(blob_store.name))"],"outputs":[{"output_type":"stream","name":"stdout","text":["agd-mlws\n","azure-ml-workshop\n","westus2\n","c5ec24ce-9c5f-4da2-bf12-9ca8e9758d60\n","Blobstore's name: workspaceblobstore\n"]}],"execution_count":2,"metadata":{"tags":["create workspace"]}},{"cell_type":"markdown","source":["### Source Directory\n","The best practice is to use separate folders for scripts and its dependent files for each step and specify that folder as the `source_directory` for the step. This helps reduce the size of the snapshot created for the step (only the specific folder is snapshotted). Since changes in any files in the `source_directory` would trigger a re-upload of the snapshot, this helps keep the reuse of the step when there are no changes in the `source_directory` of the step."],"metadata":{}},{"cell_type":"code","source":["# source directories\n","source_directory_pivot = 'src/pivot'\n","print('Pivot scripts in {} directory.'.format(source_directory_pivot))\n","\n","source_directory_join = 'src/join'\n","print('Join scripts in {} directory.'.format(source_directory_join))"],"outputs":[{"output_type":"stream","name":"stdout","text":["Pivot scripts in src/pivot directory.\n","Join scripts in src/join directory.\n"]}],"execution_count":3,"metadata":{}},{"cell_type":"markdown","source":["### Required data and script files for the tutorial\n","Sample files required to finish this tutorial are already copied to the project folder specified above. Even though the .py provided in the samples don't have much \"ML work,\" as a data scientist, you will work on this extensively as part of your work. To complete this tutorial, the contents of these files are not very important. The one-line files are for demostration purpose only."],"metadata":{}},{"cell_type":"markdown","source":["### Compute Targets\n","See the list of Compute Targets on the workspace."],"metadata":{}},{"cell_type":"code","source":["cts = ws.compute_targets\n","for ct in cts:\n","    print(ct)"],"outputs":[{"output_type":"stream","name":"stdout","text":["agd-inference\n","agd-inference-v\n","agd-training-cpu\n","agd-training-gpu\n"]}],"execution_count":4,"metadata":{}},{"cell_type":"markdown","source":["#### Retrieve an AML compute\n","Azure Machine Learning Compute is a service for provisioning and managing clusters of Azure virtual machines for running machine learning workloads. Let's get the default Aml Compute in the current workspace. We will then run the training script on this compute target."],"metadata":{}},{"cell_type":"code","source":["from azureml.core.compute_target import ComputeTargetException\n","\n","aml_compute_target = \"agd-training-cpu\"\n","try:\n","    aml_compute = AmlCompute(ws, aml_compute_target)\n","    print(\"found existing compute target.\")\n","    print(\"AML compute target attached\")\n","except ComputeTargetException:\n","    print(\"compute target not found\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["found existing compute target.\n","AML compute target attached\n"]}],"execution_count":5,"metadata":{}},{"cell_type":"code","source":["# For a more detailed view of current Azure Machine Learning Compute status, use get_status()\n","# example: un-comment the following line.\n","print(aml_compute.get_status().serialize())"],"outputs":[{"output_type":"stream","name":"stdout","text":["{'currentNodeCount': 2, 'targetNodeCount': 2, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 2, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2020-03-30T21:23:00.195000+00:00', 'errors': None, 'creationTime': '2020-03-09T22:37:36.770656+00:00', 'modifiedTime': '2020-03-30T21:11:23.007088+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT3600S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_D3_V2'}\n"]}],"execution_count":6,"metadata":{}},{"cell_type":"markdown","source":["**Wait for this call to finish before proceeding (you will see the asterisk turning to a number).**\n","\n","Now that you have created the compute target, let's see what the workspace's compute_targets() function returns. You should now see one entry named 'amlcompute' of type AmlCompute."],"metadata":{}},{"cell_type":"markdown","source":["## Building Pipeline Steps with Inputs and Outputs\n","As mentioned earlier, a step in the pipeline can take data as input. This data can be a data source that lives in one of the accessible data locations, or intermediate data produced by a previous step in the pipeline.\n","\n","### Datasources\n","Datasource is represented by **[DataReference](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.data_reference.datareference?view=azure-ml-py)** object and points to data that lives in or is accessible from Datastore. DataReference could be a pointer to a file or a directory."],"metadata":{}},{"cell_type":"code","source":["# Reference the data uploaded to blob storage using DataReference\n","# Assign the datasource to blob_input_data variable\n","\n","# DataReference(datastore, \n","#               data_reference_name=None, \n","#               path_on_datastore=None, \n","#               mode='mount', \n","#               path_on_compute=None, \n","#               overwrite=False)\n","\n","input_time_series_1 = DataReference(\n","    datastore=blob_store,\n","    data_reference_name=\"time_series_1\",\n","    path_on_datastore=\"datasets/time-series/time_series_1.csv\")\n","print(\"DataReference input_time_series_1 created\")\n","\n","input_time_series_2 = DataReference(\n","    datastore=blob_store,\n","    data_reference_name=\"time_series_2\",\n","    path_on_datastore=\"datasets/time-series/time_series_2.csv\")\n","print(\"DataReference input_time_series_2 created\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["DataReference input_time_series_1 created\n","DataReference input_time_series_2 created\n"]}],"execution_count":10,"metadata":{}},{"cell_type":"markdown","source":["### Intermediate/Output Data\n","Intermediate data (or output of a Step) is represented by **[PipelineData](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinedata?view=azure-ml-py)** object. PipelineData can be produced by one step and consumed in another step by providing the PipelineData object as an output of one step and the input of one or more steps.\n","\n","#### Constructing PipelineData\n","- **name:** [*Required*] Name of the data item within the pipeline graph\n","- **datastore_name:** Name of the Datastore to write this output to\n","- **output_name:** Name of the output\n","- **output_mode:** Specifies \"upload\" or \"mount\" modes for producing output (default: mount)\n","- **output_path_on_compute:** For \"upload\" mode, the path to which the module writes this output during execution\n","- **output_overwrite:** Flag to overwrite pre-existing data"],"metadata":{}},{"cell_type":"code","source":["# Define intermediate data using PipelineData\n","# Syntax\n","\n","# PipelineData(name, \n","#              datastore=None, \n","#              output_name=None, \n","#              output_mode='mount', \n","#              output_path_on_compute=None, \n","#              output_overwrite=None, \n","#              data_type=None, \n","#              is_directory=None)\n","\n","# Creating intermediate data definitions\n","output_time_series_1 = PipelineData(\"output_time_series_1\",datastore=blob_store)\n","print(\"PipelineData output_time_series_1 created\")\n","\n","output_time_series_2 = PipelineData(\"output_time_series_2\",datastore=blob_store)\n","print(\"PipelineData output_time_series_2 created\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["PipelineData output_time_series_1 created\n","PipelineData output_time_series_2 created\n"]}],"execution_count":11,"metadata":{}},{"cell_type":"markdown","source":["### Pipelines steps using datasources and intermediate data\n","Machine learning pipelines can have many steps and these steps could use or reuse datasources and intermediate data. Here's how we construct such a pipeline:"],"metadata":{}},{"cell_type":"markdown","source":["#### Define a Step that consumes a datasource and produces intermediate data.\n","In this step, we define a step that consumes a datasource and produces intermediate data.\n","\n","**Open `pivot.py` in the local machine and examine the arguments, inputs, and outputs for the script. That will give you a good sense of why the script argument names used below are important.** "],"metadata":{}},{"cell_type":"markdown","source":["#### Specify conda dependencies and a base docker image through a RunConfiguration\n","\n","This step uses a docker image, use a [**RunConfiguration**](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.runconfiguration?view=azure-ml-py) to specify these requirements and use when creating the PythonScriptStep. "],"metadata":{}},{"cell_type":"code","source":["from azureml.core.runconfig import RunConfiguration\n","from azureml.core.conda_dependencies import CondaDependencies\n","from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n","\n","# create a new runconfig object\n","run_config = RunConfiguration()\n","\n","# enable Docker \n","run_config.environment.docker.enabled = True\n","\n","# set Docker base image to the default CPU-based image\n","run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n","\n","# use conda_dependencies.yml to create a conda environment in the Docker image for execution\n","run_config.environment.python.user_managed_dependencies = False\n","\n","# specify CondaDependencies obj\n","#run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['scikit-learn'])"],"outputs":[],"execution_count":12,"metadata":{}},{"cell_type":"code","source":["# step4 consumes the datasource (Datareference) in the previous step\n","# and produces processed_data1\n","time_series_1_pivot_step = PythonScriptStep(\n","    script_name=\"pivot.py\", \n","    arguments=[\"--input_data\", input_time_series_1, \"--output_data\", output_time_series_1],\n","    inputs=[input_time_series_1],\n","    outputs=[output_time_series_1],\n","    compute_target=aml_compute, \n","    source_directory=source_directory_pivot,\n","    runconfig=run_config\n",")\n","print(\"PythonScriptStep time_series_1_pivot_step created\")\n","\n","time_series_2_pivot_step = PythonScriptStep(\n","    script_name=\"pivot.py\", \n","    arguments=[\"--input_data\", input_time_series_2, \"--output_data\", output_time_series_2],\n","    inputs=[input_time_series_2],\n","    outputs=[output_time_series_2],\n","    compute_target=aml_compute, \n","    source_directory=source_directory_pivot,\n","    runconfig=run_config\n",")\n","print(\"PythonScriptStep time_series_2_pivot_step created\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["PythonScriptStep time_series_1_pivot_step created\n","PythonScriptStep time_series_2_pivot_step created\n"]}],"execution_count":14,"metadata":{}},{"cell_type":"markdown","source":["#### Define a Step that consumes intermediate data and produces intermediate data\n","In this step, we define a step that consumes an intermediate data and produces intermediate data.\n","\n","**Open `join.py` in the local machine and examine the arguments, inputs, and outputs for the script. That will give you a good sense of why the script argument names used below are important.** "],"metadata":{}},{"cell_type":"code","source":["# step5 to use the intermediate data produced by step4\n","# This step also produces an output join_output_data\n","output_joined_time_series = PipelineData(\"output_joined_time_series\", datastore=blob_store)\n","\n","join_step = PythonScriptStep(\n","    script_name=\"join.py\",\n","    arguments=[\"--input_data_1\", output_time_series_1, \"--input_data_2\", output_time_series_2, \"--output_data\", output_joined_time_series],\n","    inputs=[output_time_series_1,output_time_series_2],\n","    outputs=[output_joined_time_series],\n","    compute_target=aml_compute, \n","    source_directory=source_directory_join)\n","print(\"PythonScriptStep join_step created\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["PythonScriptStep join_step created\n"]}],"execution_count":15,"metadata":{}},{"cell_type":"markdown","source":["#### Build the pipeline"],"metadata":{}},{"cell_type":"code","source":["pipeline = Pipeline(workspace=ws, steps=[join_step])\n","print (\"Pipeline is built\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["Pipeline is built\n"]}],"execution_count":16,"metadata":{}},{"cell_type":"code","source":["pipeline_run = Experiment(ws, 'use-case-1-data-prep').submit(pipeline)\n","print(\"Pipeline is submitted for execution\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["Created step join.py [d8e54cf2][66ea111b-7495-4fd0-b781-0f5924cf30ec], (This step will run and generate new outputs)\n","Created step pivot.py [ab44cb41][263dfce7-8640-48be-b87e-f887702d3e04], (This step will run and generate new outputs)Created step pivot.py [90acf336][c2073419-2da8-4344-a9c2-7723aae36a23], (This step will run and generate new outputs)\n","\n","Created data reference time_series_1 for StepId [e3e30525][89da74c8-910e-4467-92bd-f1fe7b4ba59b], (Consumers of this data will generate new runs.)\n","Created data reference time_series_2 for StepId [2ec31e49][4a9d92fd-08fd-4d76-8b97-67f52e04edec], (Consumers of this data will generate new runs.)\n","Submitted PipelineRun 559f07b2-84e5-4ac5-8fc7-1d71258dc6cd\n","Link to Azure Machine Learning studio: https://ml.azure.com/experiments/use-case-1-data-prep/runs/559f07b2-84e5-4ac5-8fc7-1d71258dc6cd?wsid=/subscriptions/c5ec24ce-9c5f-4da2-bf12-9ca8e9758d60/resourcegroups/azure-ml-workshop/workspaces/agd-mlws\n","Pipeline is submitted for execution\n"]}],"execution_count":17,"metadata":{}},{"cell_type":"code","source":["RunDetails(pipeline_run).show()"],"outputs":[{"output_type":"display_data","data":{"text/plain":["_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', …"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f09d09a73f9843f7a67558e36d39bebb"}},"metadata":{}},{"output_type":"display_data","data":{"application/aml.mini.widget.v1":["{\"status\": \"Running\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/use-case-1-data-prep/runs/559f07b2-84e5-4ac5-8fc7-1d71258dc6cd?wsid=/subscriptions/c5ec24ce-9c5f-4da2-bf12-9ca8e9758d60/resourcegroups/azure-ml-workshop/workspaces/agd-mlws\", \"run_id\": \"559f07b2-84e5-4ac5-8fc7-1d71258dc6cd\", \"run_properties\": {\"run_id\": \"559f07b2-84e5-4ac5-8fc7-1d71258dc6cd\", \"created_utc\": \"2020-03-30T22:00:04.949405Z\", \"properties\": {\"azureml.runsource\": \"azureml.PipelineRun\", \"runSource\": \"SDK\", \"runType\": \"SDK\", \"azureml.parameters\": \"{}\"}, \"tags\": {\"azureml.pipelineComponent\": \"pipelinerun\"}, \"end_time_utc\": null, \"status\": \"Running\", \"log_files\": {\"logs/azureml/executionlogs.txt\": \"https://agdmlws4361999107.blob.core.windows.net/azureml/ExperimentRun/dcid.559f07b2-84e5-4ac5-8fc7-1d71258dc6cd/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=MdmyY9HzV9ziSQeblfFE9D5aaBiLH0ApkJWvy%2FHtHqc%3D&st=2020-03-30T21%3A51%3A12Z&se=2020-03-31T06%3A01%3A12Z&sp=r\", \"logs/azureml/stderrlogs.txt\": \"https://agdmlws4361999107.blob.core.windows.net/azureml/ExperimentRun/dcid.559f07b2-84e5-4ac5-8fc7-1d71258dc6cd/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=iz%2FDJqSfQ%2B%2FaLb9Wa78wxslTWSn1eBbdNj%2FTiBtTBwI%3D&st=2020-03-30T21%3A51%3A12Z&se=2020-03-31T06%3A01%3A12Z&sp=r\", \"logs/azureml/stdoutlogs.txt\": \"https://agdmlws4361999107.blob.core.windows.net/azureml/ExperimentRun/dcid.559f07b2-84e5-4ac5-8fc7-1d71258dc6cd/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=rHNt5pYqclFTDcL%2FdqUm87hlo9YQahqRgTHJIbHejDU%3D&st=2020-03-30T21%3A51%3A12Z&se=2020-03-31T06%3A01%3A12Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/executionlogs.txt\", \"logs/azureml/stderrlogs.txt\", \"logs/azureml/stdoutlogs.txt\"]], \"run_duration\": \"0:01:07\"}, \"child_runs\": [{\"run_id\": \"\", \"name\": \"join.py\", \"status\": \"NotStarted\", \"start_time\": \"\", \"created_time\": \"\", \"end_time\": \"\", \"duration\": \"\"}, {\"run_id\": \"1cb62951-59b6-4a10-8f3e-ba638defabfe\", \"name\": \"pivot.py\", \"status\": \"Running\", \"start_time\": \"2020-03-30T22:00:27.693554Z\", \"created_time\": \"2020-03-30T22:00:09.053246Z\", \"end_time\": \"\", \"duration\": \"0:01:03\", \"run_number\": 8, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-03-30T22:00:09.053246Z\", \"is_reused\": \"\"}, {\"run_id\": \"7d101f85-4a2e-4ced-b605-6be6f7fafb60\", \"name\": \"pivot.py\", \"status\": \"Finished\", \"start_time\": \"2020-03-30T22:00:26.3351Z\", \"created_time\": \"2020-03-30T22:00:09.076816Z\", \"end_time\": \"2020-03-30T22:01:04.112736Z\", \"duration\": \"0:00:55\", \"run_number\": 9, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-03-30T22:00:09.076816Z\", \"is_reused\": \"\"}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2020-03-30 22:00:08Z] Submitting 2 runs, first five are: 90acf336:7d101f85-4a2e-4ced-b605-6be6f7fafb60,ab44cb41:1cb62951-59b6-4a10-8f3e-ba638defabfe\\n\", \"graph\": {\"datasource_nodes\": {\"e3e30525\": {\"node_id\": \"e3e30525\", \"name\": \"time_series_1\"}, \"2ec31e49\": {\"node_id\": \"2ec31e49\", \"name\": \"time_series_2\"}}, \"module_nodes\": {\"d8e54cf2\": {\"node_id\": \"d8e54cf2\", \"name\": \"join.py\", \"status\": \"NotStarted\"}, \"ab44cb41\": {\"node_id\": \"ab44cb41\", \"name\": \"pivot.py\", \"status\": \"Running\", \"_is_reused\": false, \"run_id\": \"1cb62951-59b6-4a10-8f3e-ba638defabfe\"}, \"90acf336\": {\"node_id\": \"90acf336\", \"name\": \"pivot.py\", \"status\": \"Finished\", \"_is_reused\": false, \"run_id\": \"7d101f85-4a2e-4ced-b605-6be6f7fafb60\"}}, \"edges\": [{\"source_node_id\": \"ab44cb41\", \"source_node_name\": \"pivot.py\", \"source_name\": \"output_time_series_1\", \"target_name\": \"output_time_series_1\", \"dst_node_id\": \"d8e54cf2\", \"dst_node_name\": \"join.py\"}, {\"source_node_id\": \"90acf336\", \"source_node_name\": \"pivot.py\", \"source_name\": \"output_time_series_2\", \"target_name\": \"output_time_series_1\", \"dst_node_id\": \"d8e54cf2\", \"dst_node_name\": \"join.py\"}, {\"source_node_id\": \"e3e30525\", \"source_node_name\": \"time_series_1\", \"source_name\": \"data\", \"target_name\": \"time_series_1\", \"dst_node_id\": \"ab44cb41\", \"dst_node_name\": \"pivot.py\"}, {\"source_node_id\": \"2ec31e49\", \"source_node_name\": \"time_series_2\", \"source_name\": \"data\", \"target_name\": \"time_series_2\", \"dst_node_id\": \"90acf336\", \"dst_node_name\": \"pivot.py\"}], \"child_runs\": [{\"run_id\": \"\", \"name\": \"join.py\", \"status\": \"NotStarted\", \"start_time\": \"\", \"created_time\": \"\", \"end_time\": \"\", \"duration\": \"\"}, {\"run_id\": \"1cb62951-59b6-4a10-8f3e-ba638defabfe\", \"name\": \"pivot.py\", \"status\": \"Running\", \"start_time\": \"2020-03-30T22:00:27.693554Z\", \"created_time\": \"2020-03-30T22:00:09.053246Z\", \"end_time\": \"\", \"duration\": \"0:01:03\", \"run_number\": 8, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-03-30T22:00:09.053246Z\", \"is_reused\": \"\"}, {\"run_id\": \"7d101f85-4a2e-4ced-b605-6be6f7fafb60\", \"name\": \"pivot.py\", \"status\": \"Finished\", \"start_time\": \"2020-03-30T22:00:26.3351Z\", \"created_time\": \"2020-03-30T22:00:09.076816Z\", \"end_time\": \"2020-03-30T22:01:04.112736Z\", \"duration\": \"0:00:55\", \"run_number\": 9, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-03-30T22:00:09.076816Z\", \"is_reused\": \"\"}]}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.0.83\"}, \"loading\": false}"]},"metadata":{}}],"execution_count":18,"metadata":{}},{"cell_type":"markdown","source":["#### Wait for pipeline run to complete"],"metadata":{}},{"cell_type":"code","source":["pipeline_run.wait_for_completion(show_output=True)"],"outputs":[{"output_type":"stream","name":"stdout","text":["PipelineRunId: 559f07b2-84e5-4ac5-8fc7-1d71258dc6cd\n","Link to Portal: https://ml.azure.com/experiments/use-case-1-data-prep/runs/559f07b2-84e5-4ac5-8fc7-1d71258dc6cd?wsid=/subscriptions/c5ec24ce-9c5f-4da2-bf12-9ca8e9758d60/resourcegroups/azure-ml-workshop/workspaces/agd-mlws\n","PipelineRun Status: Running\n","\n","\n","StepRunId: 1cb62951-59b6-4a10-8f3e-ba638defabfe\n","Link to Portal: https://ml.azure.com/experiments/use-case-1-data-prep/runs/1cb62951-59b6-4a10-8f3e-ba638defabfe?wsid=/subscriptions/c5ec24ce-9c5f-4da2-bf12-9ca8e9758d60/resourcegroups/azure-ml-workshop/workspaces/agd-mlws\n","StepRun( pivot.py ) Status: Running\n","\n","Streaming azureml-logs/65_job_prep-tvmps_5a0048198f600b89a776e9215379629de230874e2e10c24d8f28f64275377801_d.txt\n","===============================================================================================================\n","bash: /azureml-envs/azureml_1b417bb747e35859ebf611fb43071e9c/lib/libtinfo.so.5: no version information available (required by bash)\n","Starting job preparation. Current time:2020-03-30T22:00:46.024312\n","Extracting the control code.\n","fetching and extracting the control code on master node.\n","Retrieving project from snapshot: bd6444f1-60ee-4a1e-8ce4-7e3e3ec76c87\n","Starting the daemon thread to refresh tokens in background for process with pid = 98\n","Starting project file download.\n","Finished project file download.\n","Download from datastores if requested.\n","Acquired lockfile /tmp/1cb62951-59b6-4a10-8f3e-ba638defabfe-datastore.lock to downloading input data references\n","Download or mount from datasets if requested.\n","Job preparation is complete. Current time:2020-03-30T22:00:49.784159\n","\n","Streaming azureml-logs/70_driver_log.txt\n","========================================\n","bash: /azureml-envs/azureml_1b417bb747e35859ebf611fb43071e9c/lib/libtinfo.so.5: no version information available (required by bash)\n","bash: /azureml-envs/azureml_1b417bb747e35859ebf611fb43071e9c/lib/libtinfo.so.5: no version information available (required by bash)\n","Starting the daemon thread to refresh tokens in background for process with pid = 156\n","Entering Run History Context Manager.\n","Preparing to call script [ pivot.py ] with arguments: ['--input_data', '/mnt/batch/tasks/shared/LS_root/jobs/agd-mlws/azureml/1cb62951-59b6-4a10-8f3e-ba638defabfe/mounts/workspaceblobstore/datasets/time-series/time_series_1.csv', '--output_data', '/mnt/batch/tasks/shared/LS_root/jobs/agd-mlws/azureml/1cb62951-59b6-4a10-8f3e-ba638defabfe/mounts/workspaceblobstore/azureml/1cb62951-59b6-4a10-8f3e-ba638defabfe/output_time_series_1']\n","After variable expansion, calling script [ pivot.py ] with arguments: ['--input_data', '/mnt/batch/tasks/shared/LS_root/jobs/agd-mlws/azureml/1cb62951-59b6-4a10-8f3e-ba638defabfe/mounts/workspaceblobstore/datasets/time-series/time_series_1.csv', '--output_data', '/mnt/batch/tasks/shared/LS_root/jobs/agd-mlws/azureml/1cb62951-59b6-4a10-8f3e-ba638defabfe/mounts/workspaceblobstore/azureml/1cb62951-59b6-4a10-8f3e-ba638defabfe/output_time_series_1']\n","\n","Argument 1: /mnt/batch/tasks/shared/LS_root/jobs/agd-mlws/azureml/1cb62951-59b6-4a10-8f3e-ba638defabfe/mounts/workspaceblobstore/datasets/time-series/time_series_1.csv\n","Argument 2: /mnt/batch/tasks/shared/LS_root/jobs/agd-mlws/azureml/1cb62951-59b6-4a10-8f3e-ba638defabfe/mounts/workspaceblobstore/azureml/1cb62951-59b6-4a10-8f3e-ba638defabfe/output_time_series_1\n","/mnt/batch/tasks/shared/LS_root/jobs/agd-mlws/azureml/1cb62951-59b6-4a10-8f3e-ba638defabfe/mounts/workspaceblobstore/azureml/1cb62951-59b6-4a10-8f3e-ba638defabfe/output_time_series_1 created\n","\n","\n","The experiment completed successfully. Finalizing run...\n","Cleaning up all outstanding Run operations, waiting 300.0 seconds\n","1 items cleaning up...\n","Cleanup took 0.0009083747863769531 seconds\n","Starting the daemon thread to refresh tokens in background for process with pid = 156\n","\n","Streaming azureml-logs/75_job_post-tvmps_5a0048198f600b89a776e9215379629de230874e2e10c24d8f28f64275377801_d.txt\n","===============================================================================================================\n","bash: /azureml-envs/azureml_1b417bb747e35859ebf611fb43071e9c/lib/libtinfo.so.5: no version information available (required by bash)\n","Starting job release. Current time:2020-03-30T22:01:06.294955\n","Logging experiment finalizing status in history service.\n","Starting the daemon thread to refresh tokens in background for process with pid = 186\n","Job release is complete. Current time:2020-03-30T22:01:09.181944\n"]}],"execution_count":19,"metadata":{}},{"cell_type":"markdown","source":["### See Outputs\n","\n","See where outputs of each pipeline step are located on your datastore.\n","\n","***Wait for pipeline run to complete, to make sure all the outputs are ready***"],"metadata":{}},{"cell_type":"code","source":["# Get Steps\n","for step in pipeline_run.get_steps():\n","    print(\"Outputs of step \" + step.name)\n","    \n","    # Get a dictionary of StepRunOutputs with the output name as the key \n","    output_dict = step.get_outputs()\n","    \n","    for name, output in output_dict.items():\n","        \n","        output_reference = output.get_port_data_reference() # Get output port data reference\n","        print(\"\\tname: \" + name)\n","        print(\"\\tdatastore: \" + output_reference.datastore_name)\n","        print(\"\\tpath on datastore: \" + output_reference.path_on_datastore)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Outputs of step join.py\n","\tname: output_joined_time_series\n","\tdatastore: workspaceblobstore\n","\tpath on datastore: azureml/64722ebe-95d5-4a93-b7be-016cc7d2b895/output_joined_time_series\n","Outputs of step pivot.py\n","\tname: output_time_series_1\n","\tdatastore: workspaceblobstore\n","\tpath on datastore: azureml/1cb62951-59b6-4a10-8f3e-ba638defabfe/output_time_series_1\n","Outputs of step pivot.py\n","\tname: output_time_series_2\n","\tdatastore: workspaceblobstore\n","\tpath on datastore: azureml/7d101f85-4a2e-4ced-b605-6be6f7fafb60/output_time_series_2\n"]}],"execution_count":20,"metadata":{}},{"cell_type":"markdown","source":["### Download Outputs\n","\n","We can download the output of any step to our local machine using the SDK."],"metadata":{}},{"cell_type":"code","source":["# Retrieve the step runs by name 'train.py'\n","train_step = pipeline_run.find_step_run('join.py')\n","\n","if train_step:\n","    train_step_obj = train_step[0] # since we have only one step by name 'train.py'\n","    train_step_obj.get_output_data('output_data').download(\"./outputs\") # download the output to current directory"],"outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":["# Next: Publishing the Pipeline and calling it from the REST endpoint\n","See this [notebook](https://aka.ms/pl-pub-rep) to understand how the pipeline is published and you can call the REST endpoint to run the pipeline."],"metadata":{}}],"metadata":{"order_index":2,"exclude_from_index":false,"task":"Demonstrates how to construct a Pipeline with data dependency between steps","deployment":["None"],"authors":[{"name":"sanpil"}],"star_tag":["featured"],"kernel_info":{"name":"python3"},"language_info":{"name":"python","version":"3.6.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"compute":["AML Compute"],"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"tags":["None"],"datasets":["Custom"],"categories":["how-to-use-azureml","machine-learning-pipelines","intro-to-pipelines"],"category":"tutorial","framework":["Azure ML"],"friendly_name":"Azure Machine Learning Pipelines with Data Dependency","nteract":{"version":"nteract-front-end@1.0.0"}},"nbformat":4,"nbformat_minor":2}