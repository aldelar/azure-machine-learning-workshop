{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Machine Learning Data Prep\n",
    "In this notebook, we will see how we can do data prep using the azureml.dataprep SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Machine Learning and Pipeline SDK-specific Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core ML to create exeperiments and run them\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Datastore, Dataset\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "# AzureML Dataprep SDK\n",
    "import azureml.dataprep as dprep\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"AML Core SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting AML Workspace and Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create workspace"
    ]
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(\"== Workspace:\")\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n",
    "\n",
    "# Default datastore (Azure blob storage)\n",
    "# def_blob_store = ws.get_default_datastore()\n",
    "blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "print(\"== Datastore: {}\".format(blob_store.name))\n",
    "\n",
    "# list compute targets\n",
    "print(\"== Compute targets:\")\n",
    "for ct in ws.compute_targets:\n",
    "    print(\"  \" + ct)\n",
    "    \n",
    "# Retrieve a compute target    \n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "aml_compute_target = \"agd-training-cpu\"\n",
    "try:\n",
    "    aml_compute = AmlCompute(ws, aml_compute_target)\n",
    "    print(\"== AML compute target attached: \" + aml_compute_target)\n",
    "except ComputeTargetException:\n",
    "    print(\"== AML compute target not found: \" + aml_compute_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data inputs definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs definitions\n",
    "h_ts_1_dr = DataReference(datastore=blob_store,data_reference_name=\"h_ts_1\",path_on_datastore=\"datasets/time-series/S_ACTUALS.csv\")\n",
    "h_ts_2_dr = DataReference(datastore=blob_store,data_reference_name=\"h_ts_2\",path_on_datastore=\"datasets/time-series/W_ACTUALS.csv\")\n",
    "h_ts_3_dr = DataReference(datastore=blob_store,data_reference_name=\"h_ts_3\",path_on_datastore=\"datasets/time-series/C_LOAD.csv\")\n",
    "d_ts_1_dr = DataReference(datastore=blob_store,data_reference_name=\"d_ts_1\",path_on_datastore=\"datasets/time-series/X1.csv\")\n",
    "d_ts_2_dr = DataReference(datastore=blob_store,data_reference_name=\"d_ts_2\",path_on_datastore=\"datasets/time-series/X2.csv\")\n",
    "\n",
    "print(\"== Datasets defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep using Azure ML Dataprep SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data as azureml.dataprep.DataFlow objects\n",
    "h_ts_1_dflow = dprep.read_csv(h_ts_1_dr)\n",
    "h_ts_2_dflow = dprep.read_csv(h_ts_2_dr)\n",
    "h_ts_3_dflow = dprep.read_csv(h_ts_3_dr)\n",
    "d_ts_1_dflow = dprep.read_csv(d_ts_1_dr)\n",
    "d_ts_2_dflow = dprep.read_csv(d_ts_2_dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_ts_1_dflow.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_ts_1_dflow.get_profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discover data types\n",
    "builder = h_ts_1_dflow.builders.set_column_types()\n",
    "builder.learn()\n",
    "builder.conversion_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resolve MYDATE type ambiguity\n",
    "builder.conversion_candidates['MYDATE'] = (dprep.FieldType.DATE, ['%m/%d/%Y'])\n",
    "h_ts_1_dflow = builder.to_dataflow()\n",
    "h_ts_1_dflow.get_profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead 'auto_read_file' will do type auto detect (when it can), and other things (auto skip first rows, will find the right header, etc)\n",
    "h_ts_1_dflow = dprep.auto_read_file(h_ts_1_dr)\n",
    "h_ts_1_dflow.get_profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# azureml-dataprep-sdk.py (WIP)\n",
    "# =======================\n",
    "\n",
    "h_ts_1_dflow = dprep.auto_read_file(h_ts_1_dr)\n",
    "h_ts_2_dflow = dprep.auto_read_file(h_ts_2_dr)\n",
    "h_ts_3_dflow = dprep.auto_read_file(h_ts_3_dr)\n",
    "d_ts_1_dflow = dprep.auto_read_file(d_ts_1_dr)\n",
    "d_ts_2_dflow = dprep.auto_read_file(d_ts_2_dr)\n",
    "\n",
    "# ===========================\n",
    "# EOF azureml-dataprep-sdk.py (WIP)\n",
    "# ==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts_1 and ts_2 need to be pivoted\n",
    "h_ts_1_dflow.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIVOT data\n",
    "h_ts_1_pivot_dflow = h_ts_1_dflow.pivot(['NODE_ID'],'MW',\n",
    "                                        azureml.dataprep.api.engineapi.typedefinitions.SummaryFunction.MAX,\n",
    "                                        ['MYDATE','HOUR'])\n",
    "h_ts_1_pivot_dflow.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using derive_column_by_example to reformat DATE/HOUR as a DATETIME field\n",
    "builder = h_ts_1_pivot_dflow.builders.derive_column_by_example(source_columns = ['MYDATE','HOUR'], new_column_name = 'DATETIME')\n",
    "builder.add_example(source_data = {'MYDATE': '1/1/2012', 'HOUR': 1}, example_value = '01/01/2012 01:00')\n",
    "builder.add_example(source_data = {'MYDATE': '10/10/2012', 'HOUR': 15}, example_value = '10/10/2012 15:00')\n",
    "builder.add_example(source_data = {'MYDATE': '1/17/2012', 'HOUR': 12}, example_value = '01/17/2012 12:00')\n",
    "builder.preview(skip=3000,count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_ts_1_pivot_dt_dflow = builder.to_dataflow()\n",
    "h_ts_1_pivot_dt_dflow.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_ts_1_pivot_dt_dflow.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_ts_1_pivot_dt_dflow = h_ts_1_pivot_dflow.derive_column_by_example(\n",
    "    source_columns = ['MYDATE','HOUR'],\n",
    "    new_column_name = 'DATETIME',\n",
    "    example_data = [({'MYDATE': '1/1/2012', 'HOUR': '1'},    '01/01/2012 01:00'),\n",
    "                    ({'MYDATE': '10/10/2012', 'HOUR': '15'}, '10/10/2012 15:00'),\n",
    "                    ({'MYDATE': '1/17/2012', 'HOUR': '12'},  '01/17/2012 12:00')]\n",
    "    ).drop_columns(['MYDATE','HOUR'])\n",
    "h_ts_1_pivot_dt_dflow.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = h_ts_1_pivot_dt_dflow.builders.set_column_types()\n",
    "builder.learn()\n",
    "builder.conversion_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.conversion_candidates['DATETIME'] = (dprep.FieldType.DATE, ['%d/%m/%Y %H:%M'])\n",
    "h_ts_1_pivot_dt_dflow = builder.to_dataflow()\n",
    "h_ts_1_pivot_dt_dflow.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_ts_1_pivot_dt_dflow.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_ts_1_pivot_dt_dflow.get_profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# azureml-dataprep-sdk.py\n",
    "# =======================\n",
    "\n",
    "h_ts_1_dflow = dprep.auto_read_file(h_ts_1_dr)\n",
    "h_ts_2_dflow = dprep.auto_read_file(h_ts_2_dr)\n",
    "h_ts_3_dflow = dprep.auto_read_file(h_ts_3_dr)\n",
    "d_ts_1_dflow = dprep.auto_read_file(d_ts_1_dr)\n",
    "d_ts_2_dflow = dprep.auto_read_file(d_ts_2_dr)\n",
    "\n",
    "# Pivot data\n",
    "h_ts_1_pivot_dflow = h_ts_1_dflow.pivot(['NODE_ID'],'MW',\n",
    "                                        azureml.dataprep.api.engineapi.typedefinitions.SummaryFunction.MAX,\n",
    "                                        ['MYDATE','HOUR'])\n",
    "h_ts_2_pivot_dflow = h_ts_2_dflow.pivot(['NODE_ID'],'MW',\n",
    "                                        azureml.dataprep.api.engineapi.typedefinitions.SummaryFunction.MAX,\n",
    "                                        ['MYDATE','HOUR'])\n",
    "\n",
    "# merge DATE and HOUR and update datatype for DATETIME column\n",
    "def ts_merge_date_hour_to_datetime(dflow, date_column_name, hour_column_name):\n",
    "    # merge columns\n",
    "    dflow = dflow.derive_column_by_example(\n",
    "        source_columns = [date_column_name,hour_column_name],\n",
    "        new_column_name = 'DATETIME',\n",
    "        example_data = [({date_column_name: '1/1/2012', hour_column_name: '1'},    '01/01/2012 01:00'),\n",
    "                        ({date_column_name: '10/10/2012', hour_column_name: '15'}, '10/10/2012 15:00'),\n",
    "                        ({date_column_name: '1/17/2012', hour_column_name: '12'},  '01/17/2012 12:00')]\n",
    "        ).drop_columns([date_column_name,hour_column_name])\n",
    "    # update data type\n",
    "    builder = dflow.builders.set_column_types()\n",
    "    builder.learn()\n",
    "    builder.conversion_candidates['DATETIME'] = (dprep.FieldType.DATE, ['%d/%m/%Y %H:%M'])\n",
    "    return builder.to_dataflow()\n",
    "\n",
    "# generate all DATETIME columns with proper data type\n",
    "h_ts_1_pivot_dt_dflow = ts_merge_date_hour_to_datetime(h_ts_1_pivot_dflow,'MYDATE','HOUR')\n",
    "h_ts_2_pivot_dt_dflow = ts_merge_date_hour_to_datetime(h_ts_2_pivot_dflow,'MYDATE','HOUR')\n",
    "h_ts_3_dt_dflow =       ts_merge_date_hour_to_datetime(h_ts_3_dflow,      'DATE','HE')\n",
    "\n",
    "# ===========================\n",
    "# EOF azureml-dataprep-sdk.py (WIP)\n",
    "# ==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check profile\n",
    "h_ts_1_pivot_dt_dflow.get_profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check profile\n",
    "d_ts_1_dflow.get_profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JOINING DATA SET h_ts_j1=h1,h2\n",
    "h_ts_j1_dflow = dprep.Dataflow.join(\n",
    "    h_ts_1_pivot_dt_dflow,\n",
    "    h_ts_2_pivot_dt_dflow,\n",
    "    join_key_pairs=[('DATETIME', 'DATETIME')],\n",
    "    left_column_prefix='h1_',\n",
    "    right_column_prefix='h2_'\n",
    ").drop_columns('h2_DATETIME').rename_columns({'h1_DATETIME':'DATETIME'})\n",
    "h_ts_j1_dflow.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JOINING DATA SET h_ts_dflow=h_ts_j1,h3\n",
    "h_ts_dflow = dprep.Dataflow.join(\n",
    "    h_ts_j1_dflow,\n",
    "    h_ts_3_dt_dflow,\n",
    "    join_key_pairs=[('DATETIME', 'DATETIME')],\n",
    "    left_column_prefix='',\n",
    "    right_column_prefix='h3_'\n",
    ").drop_columns('h3_DATETIME')\n",
    "h_ts_dflow.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# azureml-dataprep-sdk.py (WIP)\n",
    "# =======================\n",
    "\n",
    "h_ts_1_dflow = dprep.auto_read_file(h_ts_1_dr)\n",
    "h_ts_2_dflow = dprep.auto_read_file(h_ts_2_dr)\n",
    "h_ts_3_dflow = dprep.auto_read_file(h_ts_3_dr)\n",
    "d_ts_1_dflow = dprep.auto_read_file(d_ts_1_dr)\n",
    "d_ts_2_dflow = dprep.auto_read_file(d_ts_2_dr)\n",
    "\n",
    "# Pivot data\n",
    "h_ts_1_pivot_dflow = h_ts_1_dflow.pivot(['NODE_ID'],'MW',\n",
    "                                        azureml.dataprep.api.engineapi.typedefinitions.SummaryFunction.MAX,\n",
    "                                        ['MYDATE','HOUR'])\n",
    "h_ts_2_pivot_dflow = h_ts_2_dflow.pivot(['NODE_ID'],'MW',\n",
    "                                        azureml.dataprep.api.engineapi.typedefinitions.SummaryFunction.MAX,\n",
    "                                        ['MYDATE','HOUR'])\n",
    "\n",
    "# merge DATE and HOUR and update datatype for DATETIME column\n",
    "def ts_merge_date_hour_to_datetime(dflow, date_column_name, hour_column_name):\n",
    "    # merge columns\n",
    "    dflow = dflow.derive_column_by_example(\n",
    "        source_columns = [date_column_name,hour_column_name],\n",
    "        new_column_name = 'DATETIME',\n",
    "        example_data = [({date_column_name: '1/1/2012', hour_column_name: '1'},    '01/01/2012 01:00'),\n",
    "                        ({date_column_name: '10/10/2012', hour_column_name: '15'}, '10/10/2012 15:00'),\n",
    "                        ({date_column_name: '1/17/2012', hour_column_name: '12'},  '01/17/2012 12:00')]\n",
    "        ).drop_columns([date_column_name,hour_column_name])\n",
    "    # update data type\n",
    "    builder = dflow.builders.set_column_types()\n",
    "    builder.learn()\n",
    "    builder.conversion_candidates['DATETIME'] = (dprep.FieldType.DATE, ['%d/%m/%Y %H:%M'])\n",
    "    return builder.to_dataflow()\n",
    "\n",
    "# generate all DATETIME columns with proper data type\n",
    "h_ts_1_pivot_dt_dflow = ts_merge_date_hour_to_datetime(h_ts_1_pivot_dflow,'MYDATE','HOUR')\n",
    "h_ts_2_pivot_dt_dflow = ts_merge_date_hour_to_datetime(h_ts_2_pivot_dflow,'MYDATE','HOUR')\n",
    "h_ts_3_dt_dflow =       ts_merge_date_hour_to_datetime(h_ts_3_dflow,      'DATE','HE')\n",
    "\n",
    "# JOINING DATA SET h_ts_dflow=join(h1,h2,h3)\n",
    "h_ts_dflow = dprep.Dataflow.join(\n",
    "    dprep.Dataflow.join(h_ts_1_pivot_dt_dflow,\n",
    "                        h_ts_2_pivot_dt_dflow,\n",
    "                        join_key_pairs=[('DATETIME', 'DATETIME')],\n",
    "                        left_column_prefix='h1_',right_column_prefix='h2_'\n",
    "                       ).drop_columns(['h2_DATETIME']).rename_columns({'h1_DATETIME':'DATETIME'}),\n",
    "    h_ts_3_dt_dflow,\n",
    "    join_key_pairs=[('DATETIME', 'DATETIME')],\n",
    "    left_column_prefix='',\n",
    "    right_column_prefix='h3_'\n",
    ").drop_columns('h3_DATETIME')\n",
    "\n",
    "# JOINING DATA SET d_ts_dflow=join(d1,d2)\n",
    "d_ts_dflow = dprep.Dataflow.join(\n",
    "    d_ts_1_dflow,\n",
    "    d_ts_2_dflow,\n",
    "    join_key_pairs=[('RDATE', 'RDATE')],\n",
    "    left_column_prefix='',\n",
    "    right_column_prefix='r_').drop_columns(['r_RDATE']).rename_columns({'r_X2':'X2'})\n",
    "\n",
    "# ===========================\n",
    "# EOF azureml-dataprep-sdk.py (WIP)\n",
    "# ==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check profile \n",
    "h_ts_dflow.get_profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ==> KEEP DATE COLUMN IN H to properly summarize by DATE and not DATETIME in H\n",
    "# TODO ==> KEEP DATE COLUMN IN H to properly summarize by DATE and not DATETIME in H\n",
    "# TODO ==> KEEP DATE COLUMN IN H to properly summarize by DATE and not DATETIME in H\n",
    "# TODO ==> KEEP DATE COLUMN IN H to properly summarize by DATE and not DATETIME in H\n",
    "# TODO ==> KEEP DATE COLUMN IN H to properly summarize by DATE and not DATETIME in H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize h_ts_dflow to daily\n",
    "def generate_summary_column(column_name,column_suffix,summary_function):\n",
    "    return dprep.SummaryColumnsValue(\n",
    "                column_id=column_name,\n",
    "                summary_column_name=column_name+'_'+column_suffix,\n",
    "                summary_function=summary_function)\n",
    "\n",
    "def generate_summary_columns(dflow):\n",
    "    summary_columns = []\n",
    "    for key in h_ts_dflow.get_profile().columns.keys():\n",
    "        if key != 'DATETIME':\n",
    "            summary_columns.append(generate_summary_column(key,'MAX',dprep.SummaryFunction.MAX))\n",
    "            summary_columns.append(generate_summary_column(key,'MIN',dprep.SummaryFunction.MIN))\n",
    "            summary_columns.append(generate_summary_column(key,'MEAN',dprep.SummaryFunction.MEAN))\n",
    "            summary_columns.append(generate_summary_column(key,'MEDIAN',dprep.SummaryFunction.MEDIAN))\n",
    "    return summary_columns\n",
    "\n",
    "h_ts_summarized_dflow = h_ts_dflow.summarize(\n",
    "    summary_columns=generate_summary_columns(h_ts_dflow),\n",
    "    group_by_columns=['DATETIME'])\n",
    "\n",
    "h_ts_summarized_dflow.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL azureml-dataprep-sdk.py Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# azureml-dataprep-sdk.py\n",
    "# =======================\n",
    "\n",
    "h_ts_1_dflow = dprep.auto_read_file(h_ts_1_dr)\n",
    "h_ts_2_dflow = dprep.auto_read_file(h_ts_2_dr)\n",
    "h_ts_3_dflow = dprep.auto_read_file(h_ts_3_dr)\n",
    "d_ts_1_dflow = dprep.auto_read_file(d_ts_1_dr)\n",
    "d_ts_2_dflow = dprep.auto_read_file(d_ts_2_dr)\n",
    "\n",
    "# Pivot data\n",
    "h_ts_1_pivot_dflow = h_ts_1_dflow.pivot(['NODE_ID'],'MW',\n",
    "                                        azureml.dataprep.api.engineapi.typedefinitions.SummaryFunction.MAX,\n",
    "                                        ['MYDATE','HOUR'])\n",
    "h_ts_2_pivot_dflow = h_ts_2_dflow.pivot(['NODE_ID'],'MW',\n",
    "                                        azureml.dataprep.api.engineapi.typedefinitions.SummaryFunction.MAX,\n",
    "                                        ['MYDATE','HOUR'])\n",
    "\n",
    "# merge DATE and HOUR and update datatype for DATETIME column\n",
    "def ts_merge_date_hour_to_datetime(dflow, date_column_name, hour_column_name):\n",
    "    # merge columns\n",
    "    dflow = dflow.derive_column_by_example(\n",
    "        source_columns = [date_column_name,hour_column_name],\n",
    "        new_column_name = 'DATETIME',\n",
    "        example_data = [({date_column_name: '1/1/2012', hour_column_name: '1'},    '01/01/2012 01:00'),\n",
    "                        ({date_column_name: '10/10/2012', hour_column_name: '15'}, '10/10/2012 15:00'),\n",
    "                        ({date_column_name: '1/17/2012', hour_column_name: '12'},  '01/17/2012 12:00')]\n",
    "        ).drop_columns([date_column_name,hour_column_name])\n",
    "    # update data type\n",
    "    builder = dflow.builders.set_column_types()\n",
    "    builder.learn()\n",
    "    builder.conversion_candidates['DATETIME'] = (dprep.FieldType.DATE, ['%d/%m/%Y %H:%M'])\n",
    "    return builder.to_dataflow()\n",
    "\n",
    "# generate all DATETIME columns with proper data type\n",
    "h_ts_1_pivot_dt_dflow = ts_merge_date_hour_to_datetime(h_ts_1_pivot_dflow,'MYDATE','HOUR')\n",
    "h_ts_2_pivot_dt_dflow = ts_merge_date_hour_to_datetime(h_ts_2_pivot_dflow,'MYDATE','HOUR')\n",
    "h_ts_3_dt_dflow =       ts_merge_date_hour_to_datetime(h_ts_3_dflow,      'DATE','HE')\n",
    "\n",
    "# JOINING DATA SET h_ts_dflow=join(h1,h2,h3)\n",
    "h_ts_dflow = dprep.Dataflow.join(\n",
    "    dprep.Dataflow.join(h_ts_1_pivot_dt_dflow,\n",
    "                        h_ts_2_pivot_dt_dflow,\n",
    "                        join_key_pairs=[('DATETIME', 'DATETIME')],\n",
    "                        left_column_prefix='h1_',right_column_prefix='h2_'\n",
    "                       ).drop_columns(['h2_DATETIME']).rename_columns({'h1_DATETIME':'DATETIME'}),\n",
    "    h_ts_3_dt_dflow,\n",
    "    join_key_pairs=[('DATETIME', 'DATETIME')],\n",
    "    left_column_prefix='',\n",
    "    right_column_prefix='h3_').drop_columns(['h3_DATETIME'])\n",
    "\n",
    "# JOINING DATA SET d_ts_dflow=join(d1,d2)\n",
    "d_ts_dflow = dprep.Dataflow.join(\n",
    "    d_ts_1_dflow,\n",
    "    d_ts_2_dflow,\n",
    "    join_key_pairs=[('RDATE', 'RDATE')],\n",
    "    left_column_prefix='',\n",
    "    right_column_prefix='r_').drop_columns(['r_RDATE']).rename_columns({'r_X2':'X2'})\n",
    "\n",
    "# helper: generate summary column\n",
    "def generate_summary_column(column_name,column_suffix,summary_function):\n",
    "    return dprep.SummaryColumnsValue(\n",
    "                column_id=column_name,\n",
    "                summary_column_name=column_name+'_'+column_suffix,\n",
    "                summary_function=summary_function)\n",
    "\n",
    "# helper: generate summary column for a few functions for each column that's not DATETIME\n",
    "def generate_summary_columns(dflow):\n",
    "    summary_columns = []\n",
    "    for key in h_ts_dflow.get_profile().columns.keys():\n",
    "        if key != 'DATETIME':\n",
    "            summary_columns.append(generate_summary_column(key,'MAX',dprep.SummaryFunction.MAX))\n",
    "            summary_columns.append(generate_summary_column(key,'MIN',dprep.SummaryFunction.MIN))\n",
    "            summary_columns.append(generate_summary_column(key,'MEAN',dprep.SummaryFunction.MEAN))\n",
    "            summary_columns.append(generate_summary_column(key,'MEDIAN',dprep.SummaryFunction.MEDIAN))\n",
    "    return summary_columns\n",
    "\n",
    "# summarize h_ts_dflow to daily\n",
    "h_ts_summarized_dflow = h_ts_dflow.summarize(\n",
    "    summary_columns=generate_summary_columns(h_ts_dflow),\n",
    "    group_by_columns=['DATETIME'])\n",
    "\n",
    "# join h and d series\n",
    "training_dflow = dprep.Dataflow.join(\n",
    "    h_ts_summarized_dflow,\n",
    "    d_ts_dflow,\n",
    "    join_key_pairs=[('DATETIME', 'RDATE')],\n",
    "    left_column_prefix='',\n",
    "    right_column_prefix='r_').drop_columns(['r_RDATE']).rename_columns({'r_X1':'X1','r_X2':'X2'})\n",
    "\n",
    "# ===========================\n",
    "# EOF azureml-dataprep-sdk.py\n",
    "# ==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test output\n",
    "training_dflow.write_to_csv(directory_path='training_dflow.csv').run_local()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check profile\n",
    "training_dflow.get_profile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating RunConfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# create a new runconfig object\n",
    "run_config = RunConfiguration()\n",
    "\n",
    "# enable Docker \n",
    "run_config.environment.docker.enabled = True\n",
    "\n",
    "# set Docker base image to the default CPU-based image\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "\n",
    "# use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# specify dependencies\n",
    "#run_config.environment.python.conda_dependencies = CondaDependencies.create(\n",
    "#    conda_packages=['pandas'],\n",
    "#    pip_packages=['azureml-sdk', 'azureml-dataprep[fuse,pandas]', 'azureml-train-automl'], \n",
    "#    pin_sdk_version=False)\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies(\n",
    "    conda_dependencies_file_path='data-prep-pipeline.yml')\n",
    "\n",
    "#\n",
    "print(\"== Run Configuration created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best practice is to use separate folders for scripts and its dependent files\n",
    "# for each step and specify that folder as the source_directory for the step.\n",
    "# This helps reduce the size of the snapshot created for the step (only the specific folder is snapshotted).\n",
    "# Since changes in any files in the source_directory would trigger a re-upload of the snapshot, this helps\n",
    "# keep the reuse of the step when there are no changes in the source_directory of the step.\n",
    "source_directory_dataprep = 'src/azureml-dataprep-sdk'\n",
    "\n",
    "# inputs\n",
    "h_ts_1_dr = DataReference(datastore=blob_store,data_reference_name=\"h_ts_1\",path_on_datastore=\"datasets/time-series/S_ACTUALS.csv\")\n",
    "h_ts_2_dr = DataReference(datastore=blob_store,data_reference_name=\"h_ts_2\",path_on_datastore=\"datasets/time-series/W_ACTUALS.csv\")\n",
    "h_ts_3_dr = DataReference(datastore=blob_store,data_reference_name=\"h_ts_3\",path_on_datastore=\"datasets/time-series/C_LOAD.csv\")\n",
    "d_ts_1_dr = DataReference(datastore=blob_store,data_reference_name=\"d_ts_1\",path_on_datastore=\"datasets/time-series/X1.csv\")\n",
    "d_ts_2_dr = DataReference(datastore=blob_store,data_reference_name=\"d_ts_2\",path_on_datastore=\"datasets/time-series/X2.csv\")\n",
    "\n",
    "# output\n",
    "d_use_case_1_dataprep_sdk_pd = PipelineData(\"d_use_case_1_dataprep_sdk\",datastore=blob_store)\n",
    "\n",
    "# Step\n",
    "use_case_1_dataprep_sdk_step = PythonScriptStep(\n",
    "    script_name=\"azureml-dataprep-sdk.py\",\n",
    "    arguments=[ \"--h_ts_1\", h_ts_1_dr, \"--h_ts_2\", h_ts_2_dr, \"--h_ts_3\", h_ts_3_dr,\n",
    "                \"--d_ts_1\", d_ts_1_dr, \"--d_ts_2\", d_ts_2_dr,\n",
    "                \"--output\", d_use_case_1_dataprep_sdk_pd ],\n",
    "    inputs=[h_ts_1_dr,h_ts_2_dr,h_ts_3_dr,\n",
    "            d_ts_1_dr,d_ts_2_dr],\n",
    "    outputs=[d_use_case_1_dataprep_sdk_pd],\n",
    "    compute_target=aml_compute,\n",
    "    source_directory=source_directory_dataprep,\n",
    "    runconfig=run_config\n",
    ")\n",
    "\n",
    "print(\"== PythonScriptStep use_case_1_dataprep_step created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the pipeline and submit an Experiment run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[use_case_1_dataprep_sdk_step])\n",
    "print (\"== Pipeline is built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = Experiment(ws, 'use-case-1-dataprep-sdk').submit(pipeline)\n",
    "print(\"== Pipeline is submitted for execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for pipeline run to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See Outputs\n",
    "\n",
    "See where outputs of each pipeline step are located on your datastore.\n",
    "\n",
    "***Wait for pipeline run to complete, to make sure all the outputs are ready***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Steps\n",
    "for step in pipeline_run.get_steps():\n",
    "    print(\"== Outputs of step \" + step.name)\n",
    "    \n",
    "    # Get a dictionary of StepRunOutputs with the output name as the key \n",
    "    output_dict = step.get_outputs()\n",
    "    \n",
    "    for name, output in output_dict.items():\n",
    "        output_reference = output.get_port_data_reference() # Get output port data reference\n",
    "        print(\"\\tname: \" + name)\n",
    "        print(\"\\tdatastore: \" + output_reference.datastore_name)\n",
    "        print(\"\\tpath on datastore: \" + output_reference.path_on_datastore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGISTER a new version of the final output as a Dataset\n",
    "\n",
    "from azureml.core import Dataset, Datastore\n",
    "from azureml.data.datapath import DataPath\n",
    "\n",
    "# find output dataset\n",
    "for step in pipeline_run.get_steps():\n",
    "    output_dict = step.get_outputs()\n",
    "    for name, output in output_dict.items():\n",
    "        if name == 'd_use_case_1_dataprep_sdk':\n",
    "            # generate a Tabular DataSet for it\n",
    "            output_reference = output.get_port_data_reference()\n",
    "            datastore_path = [DataPath(blob_store, output_reference.path_on_datastore)]\n",
    "            ds = Dataset.Tabular.from_delimited_files(datastore_path)\n",
    "            dataset_name = 'd_use_case_1_dataprep_sdk'\n",
    "            ds.register(ws, name=dataset_name, create_new_version=True)\n",
    "            print(\"== Registered new version of dataset: \" + dataset_name)"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "sanpil"
   }
  ],
  "categories": [
   "how-to-use-azureml",
   "machine-learning-pipelines",
   "intro-to-pipelines"
  ],
  "category": "tutorial",
  "compute": [
   "AML Compute"
  ],
  "datasets": [
   "Custom"
  ],
  "deployment": [
   "None"
  ],
  "exclude_from_index": false,
  "framework": [
   "Azure ML"
  ],
  "friendly_name": "Azure Machine Learning Pipelines with Data Dependency",
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "order_index": 2,
  "star_tag": [
   "featured"
  ],
  "tags": [
   "None"
  ],
  "task": "Demonstrates how to construct a Pipeline with data dependency between steps"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
